{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fe81e4",
   "metadata": {},
   "source": [
    "# 뉴스데이터 수집\n",
    "- 뉴스의 타이틀만 수집\n",
    "- 선택자 분석법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 제목을 가져오는 함수 \n",
    "#  습관 1 : 검증후 변수에 넣기!!\n",
    "res = req.get(\"https://zdnet.co.kr/news/?lstcode=0000&page=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "561d89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. bs으로 HTML 데이터 변환\n",
    "soup = bs(res.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. html데이터에서 데이터 수집\n",
    "# 습관 2 : 개발자툴을 확인하고, 개발자도구에서 다시 영역확인하기.\n",
    "\n",
    "# 대화 : html 데이터에서 h3태그만 수집해\n",
    "## soup.find_all(\"h3\")\n",
    "## soup.select(\"h3\")\n",
    "\n",
    "# selector를 이용해서 h3 태그를 가져와야한다. 계층선택자 부모자식을 활용해서 span그룹을 제외한 data를 가져온다. \n",
    "#soup.select(\"a>h3\")\n",
    "\n",
    "# 부모도 id or class가 있을때까지 계층선택자를 진행하라\n",
    "# 클래스나 id 있는경우 복사 붙여넣기로 가져오기 !!! 오타조심 \n",
    "## soup.select(\"div.assetText>*>h3\")\n",
    "\n",
    "\n",
    "# 문제점 : 선택자의 범위가 넓어서 불필요한 데이터가 포함\n",
    "# 해결책 : 최대한 선택자의 범위를 좁히자.\n",
    "# 개념 : 내가 선택한 요소에 id, class 구분자가 없다면 부모태그를 활용하자. \n",
    "# 주의점 : 부모가 id, class를 가질 때 까지 좁힌다 =? 확률을 올리기 위해서 \n",
    "## soup.select(\"div.assetText>a>h3\")\n",
    "\n",
    "# 수집한 데이터 변수에 담기\n",
    "title = soup.select(\"div.assetText > a > h3\")\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e31fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 현재 수집된 요소(태그+속성+컨텐츠) 에서 => 필요한 제목만 추출(컨텐츠)\n",
    "\n",
    "title[0].text\n",
    "\n",
    "\n",
    "for i in range(len(title)):\n",
    "# title[i]를 넣게 된다면 tag와 content가 모두 출력된다. 따라서 title[i].text를 사용해서 content만 추출한다.\n",
    "    print(type(title[i].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ad212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 반목문을 통해서 모든 결과를 출력\n",
    "# len의 활용법 : for문의 반복횟수 사용, 데이터의 유무검증(if)\n",
    "# 비어있는 리스트에 글자만 저장할거야.\n",
    "title_list = []\n",
    "# 반복문을 통해서 title의 개수만큼 반복\n",
    "for i in range(len(title)):\n",
    "   # title[i]를 넣게 된다면 tag와 content가 모두 출력된다. 따라서 title[i].text를 사용해서 content만 추출한다.\n",
    "   title_list.append(title[i].text) \n",
    "\n",
    "title_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d79d5",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e38a5e",
   "metadata": {},
   "source": [
    "# 크롤링 1회차 정리\n",
    "\n",
    "- 1. 크롤링 : 웹상에 존재한느 데이터를 컴퓨터가 수집하는 기술\n",
    " - 컴퓨터와 대화를 한다고 생각\n",
    " - 사람이 하는 행위를 세분화 해서 명령을 내린다.\n",
    "- 2. requests 라이브러리\n",
    "  - 브라우저를 대신하는 라이브러리\n",
    "  - 파이썬에서 데이터를 요청하는 라이브러리\n",
    "  - req.get(\"url\")\n",
    "  - 핵심 : 반드시 요청이 발생하면 응답이 넘어온다.\n",
    "  - 응답 : 200(성공), 400(실패,내탓), 500(실패,서버탓)\n",
    "- 3. BeautifulSoup 라이브러리\n",
    " - string데이터를 html데이터로 변환 (parser를 통해서 parsing)\n",
    " - 핵심 : 컴퓨터는 소통하기위해서 HTML이 필요하다. \n",
    "- 4. 특정 요소를 수집\n",
    " - soup.select(\"선택자 id,class > * > h2\")\n",
    " - 선택자는 작성할 때 반드시 범위르 최대한 좁힌다.\n",
    " - 이유 : 정확한 데이터를 수집할 확률을 올린다.\n",
    " - 내가 수집할 요소가 id, class(구분자)가 없다면 => 부모자식을 활용하자.\n",
    " - 핵심 : 부모가 구분자가 있을 때 까지 검사한다 => 범위를 좁힌다. \n",
    "- 5. crowling은 모든 사이트가 프로세스는 똑같다. \n",
    " - 1) 데이터 요청\n",
    " - 2) 데이터 변환\n",
    " - 3) 데이터 수집\n",
    " - 4) 데이터 가공\n",
    " - 5) 데이터 활용\n",
    "\n",
    "- 6. 월요일에 학습 내용\n",
    " - 링크 수집 (추가) anchor 태그의 href속성 \n",
    " - 상대경로 vs 절대경로: 수집된 링크가 상대경로이다. => 절대경로로 만들어줘야한다.\n",
    " - DF DataFrame으로 변환\n",
    " - 파일로 저장\n",
    "\n",
    " - 멜론차트 406 오류해결 , 선택자 해결 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6c7cd",
   "metadata": {},
   "source": [
    "##  데이터 확장하기\n",
    "- 타이틀과 함께 같이 파일로 저장하면 좋은 데이터 = 링크\n",
    "- 링크는 anchor 태그에 존재한다 =>href (hyperText reference)속성을 수집\n",
    "- 속성 수집하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "442a7460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/view/?no=20250711151522\">\n",
       " <h3>400km 우주 상공에서 종이비행기 날렸더니…</h3>\n",
       " <p>종이로 만든 종이비행기를 국제우주정거장(ISS)에서 던지면 어떻게 될까? 이 궁금증을 해결한 일본 도쿄대학 연구진의 논문이 공개돼 주목되고 있다고 IT매체 기가진, 사이언스얼랏...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711145517\">\n",
       " <h3>바디프랜드, '모바일 라이브' 3년 연속 성장세</h3>\n",
       " <p>국내 라이브 커머스 시장이 폭발적인 성장세를 이어가는 가운데, 헬스케어로봇 기업 바디프랜드의 모바일 라이브 커머스 거래량이 최근 3년 간 순증하고 있다. 11일 바디프랜드에 따...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711154702\">\n",
       " <h3>AI시대 개인정보 보호 입법방향 토론회 열린다</h3>\n",
       " <p>국회 정무위원회 소속 국회의원 김남근, 김승원, 김용만, 김현정, 민병덕, 박범계, 박찬대, 이인영, 이정문, 허영 의원이 공동으로 주최하고 한국정보통신법학회와 한국데이터법정책...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711142503\">\n",
       " <h3>최휘영 문체장관 후보는…언론·포털 거친 플랫폼 전문가</h3>\n",
       " <p>이재명 대통령은 11일 최휘영 놀유니버스 대표를 문화체육관광부 장관 후보자로 지명했다. 최 후보자는 관료가 아닌 언론-기업인 출신으로, 플랫폼과 관광 등 문화 산업에 이해도가 ...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711152938\">\n",
       " <h3>11번가, '2025 유통상생대회' 공정거래위원장 표창 수상</h3>\n",
       " <p>11번가가 중소상공인(중소기업·소상공인) 동반성장에 앞장선 공로를 인정받아 '2025 유통상생대회'에서 공정거래위원장 표창을 받았다고 11일 밝혔다. 올해 5회를 맞은 유통상생...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711152913\">\n",
       " <h3>갤럭시Z 지원금 소비자 혼란 우려...방통위 사전 경고</h3>\n",
       " <p>방송통신위원회는 오는 22일 단통법 폐지 시행 후 처음 출시되는 삼성전자 갤럭시Z 시리즈와 관련, 대리점과 파매점에서 휴대폰 지원금에 대한 잘못된 정보로 이용자 피해가 우려된다...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711152506\">\n",
       " <h3>인디제이, '눈치 RAG 2.0' 출시…한국어 미묘한 맥락·문화적 깊이까지 이해</h3>\n",
       " <p>감성지능 AI 기술 선도 기업 인디제이(inDJ·대표 정우주)는 한국어의 미묘한 맥락과 문화적 깊이까지 이해하는 '눈치(Noonchee) RAG 2.0'을 공식 출시한다고 11...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711151336\">\n",
       " <h3>[프로필] 김윤덕 국토교통부 장관 후보자…국토위 활동 3선 국회의원</h3>\n",
       " <p>이재명 정부의 첫 번째 국토교통부 장관 후보자로 지명된 김윤덕 더불어민주당 의원은 전주 동암고와 전북대 회계학과를 졸업했다. 대학시절 민주화 운동에 앞장섰고 이후 시민행동21 ...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711151135\">\n",
       " <h3>15일부터 입국자 검역 서비스 확대된다</h3>\n",
       " <p>질병관리청이 오는 15일부터 '공항만 여행자 호흡기 감염병 검사 서비스 시범사업'과 'Q-CODE 기반 전자검역 시범사업'을 확대 시행한다. 우선 공항만 여행자 호흡기 감염병 ...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711141936\">\n",
       " <h3>\"배민 '가게 운영 노하우 교육' 덕에 배달 주문 수 늘었어요\"</h3>\n",
       " <p>배달의민족이 중소벤처기업부의 '온라인 브랜드 소상공인 육성사업(TOPS 프로그램)'을 통해 지원한 외식업 소상공인들의 배달 주문 수가 70% 증가했다. 배민 운영사 우아한형제들...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711145538\">\n",
       " <h3>내란특검 \"윤석열 조사 불출석 합당하지 않으면 강제구인\"</h3>\n",
       " <p>내란특검팀이 윤석열 전 대통령의 소환 조사 불응에 대해 사유가 합당하지 않으면 강제구인에 나서기로 했다. 박지영 내란특검팀 특별검사보는 11일 브리핑에서 “윤 전 대통령을 금일...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711145450\">\n",
       " <h3>벤츠 사회공헌위원회, 미래차 인재와 독일 본사 탐방 성료</h3>\n",
       " <p>메르세데스-벤츠 사회공헌위원회는 미래 자동차 인재 육성 지원을 위해 11명의 '제18기 메르세데스-벤츠 모바일 아카데미' 우수 수료 학생을 대상으로 지난 6월 29일부터 6박 ...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711134046\">\n",
       " <h3>갤럭시S26서 플러스 모델 사라지나</h3>\n",
       " <p>삼성전자의 차세대 플래그십 스마트폰 갤럭시S26 시리즈에서 플러스 모델이 사라질 가능성이 다시 한번 제기됐다. IT매체 샘마이폴리스는 최근 세계 이동통신사업자협회(GSMA) 데...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711143453\">\n",
       " <h3>고려아연 임직원, 혹서기 취약계층에 삼계탕 나눔</h3>\n",
       " <p>고려아연이 혹서기 취약계층을 위한 필요 물품들을 후원하고, 여름 보양식을 직접 만드는 봉사활동을 진행했다. 고려아연은 지난 10일 서울시 노원구 대한적십자사 서울지사 북부봉사관...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711141814\">\n",
       " <h3>효성벤처스, 1천억 펀드 가동…딥테크 스타트업 4곳 투자</h3>\n",
       " <p>효성의 기업주도형 벤처캐피탈(CVC) 효성벤처스가 '스타트업코리아펀드(스코펀)'의 첫 투자를 단행했다. 효성벤처스는 지난달 27일과 지난 10일에 ▲인공지능(AI) 기반 신약개...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711142256\">\n",
       " <h3>문체 최휘영·국토 김윤덕...이재명 정부 초대내각 인선 완료</h3>\n",
       " <p>이재명 대통령이 11일 문화체육관광부 장관에 최휘영 놀유니버스 대표, 국토교통부 장관에 김윤덕 더불어민주당 의원을 지명했다. 이에 따라, 대통령 취임 37일 만에 초대 내각 인...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711131825\">\n",
       " <h3>토성의 달 '엔셀라두스' 바닷물은 '강한 염기성' [우주로 간다]</h3>\n",
       " <p>지구 밖 세계 중 생명체 존재 가능성이 높다고 알려진 토성의 위성 '엔셀라두스'의 바닷물의 수소이온농도(pH) 수치가 공개됐다고 우주과학매체 스페이스닷컴이 최근 보도했다. 이번...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711135729\">\n",
       " <h3>영풍, 환경단체 낙동강 카드뮴 오염 주장에 \"근거 없는 비방\" 일축</h3>\n",
       " <p>영풍이 석포제련소 환경오염 논란을 정면 반박했다. 11일 민주사회를 위한 변호사 모임 낙동강 석포제련소 TF 및 낙동강 상류 환경 피해 주민 대책위원회는 국회에서 기자회견을 열...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711100124\">\n",
       " <h3>[포토] \"여기 왜 있어?\"…미군 비행장에 등장한 코요테 로봇</h3>\n",
       " <p>미국 육군이 위험한 야생 동물로부터 비행장을 보호하기 위해 로봇 코요테를 개발 중이라고 과학전문매체 뉴아틀라스가 최근 보도했다. 비행장과 활주로의 가장 큰 위협 대상 중 하나는...</p>\n",
       " </a>,\n",
       " <a href=\"/view/?no=20250711115246\">\n",
       " <h3>클로봇, 피지컬 AI 기술 고도화 추진</h3>\n",
       " <p>자율주행 로봇 서비스 전문기업 클로봇은 국가로봇테스트필드에 실환경 연동 디지털 트윈을 활용한 실시간 증강 실험 기술을 개발하는 과제 주관기관으로 선정돼 협약을 체결했다고 11일...</p>\n",
       " </a>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a태그를 수집\n",
    "## soup.select(\"div.assetText > a\")\n",
    "\n",
    "\n",
    "# a태그를 수잡한 것을 aTag변수에 넣는다\n",
    "## aTag = soup.select(\"div.assetText > a\")\n",
    "\n",
    "## ★★★★★중복되는숫자가 있으므로 다시 잘 가지고와야한다. 20개 VS 24개★★★★★ \n",
    "## ★★★★★사이트 마다 구조가 다르므로 선택자를 잘 찾아야한다. 중복이 안되게 조상까지 올라가야한다.★★★★★\n",
    "aTag = soup.select(\"div.news_box > div.newsPost > div.assetText > a\")\n",
    "aTag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ac3c6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/view/?no=20250711151522'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ★★★★★속성수집!!★★★★★\n",
    "# 수집한 a태그에서 href 속성을 수집한다. // 속성의에서 href를 꺼내오던지 h1 꺼내오던지 다 가능하다! \n",
    "aTag[0][\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff00dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문을 통해서 경로만 저장하는 리스트 제작\n",
    "# 나는 url_list에 데이터를 넣을건데, aTag에 있는 데이터에서 href만 하나씩 넣을꺼야.\n",
    "url_list = []\n",
    "## for i in range(len(aTag)):\n",
    "##     url_list.append(aTag[i][\"href\"])\n",
    "\n",
    "# 현재의 url_list는 상대경로이다. 따라서 절대경로로 변경해줘야한다. \n",
    "## https://zdnet.co.kr/ + 상대경로\n",
    "# 방법 : 조회한 사이트의 서버주소 (com, net, co.kr등) 뒤에 상대경로를 더해주자.\n",
    "# 속성수집!!(href, src, class, id 등등)\n",
    "for i in range(len(aTag)):\n",
    "    url_list.append(\"https://zdnet.co.kr/\"+aTag[i][\"href\"])\n",
    "\n",
    "# 갯수와 link와 갯수검증을 해야한다. \n",
    "# 핵심 : 데이터의 개수가 중요한 경우에는 반드시 개수를 검증 \n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★★★★★title 갯수와  url 갯수검증을 해야한다.★★★★★ \n",
    "# 핵심 : 데이터의 개수가 중요한 경우에는 반드시 개수를 검증\n",
    "# 선택자를 잘 찾아야한다. 중복이 안되게 조상까지 올라가야한다. \n",
    "# 해결책 : 중복되는 선택자를 찾는다. => 중복이 되지 않을 때 까지 작성한다.\n",
    "# 선택자에서 계층선택자가 매우 중요하다.\n",
    "\n",
    "print(len(title_list), len(url_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d03fb",
   "metadata": {},
   "source": [
    "## 데이터 확장하기 2\n",
    "- 현재는 한페이지에서만 데이터를 수집 가능(20개)\n",
    "- 크롤링은 많은 양의 데이터를 수집할 때 효율적\n",
    "- 다음 페이지를 수집하는 미션\n",
    "- 1) 2페이지를 클릭 (req)로는 불가능하다. => 조작이 안되기 때문에.\n",
    "- 2) url의 변하는 값을 활용한다. https://zdnet.co.kr/news/?lstcode=0000&page=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2aa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 2페이지의 url로 사이트 요청하기 => req.get()\n",
    "res = req.get(\"https://zdnet.co.kr/news/?lstcode=0000&page=2\")\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b5987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. parsing  bs으로 데이터 파싱하기 (response의 내용들 text,tag,element > lxml parser이용해서  => html로 parsing)\n",
    "soup2 = bs(res.text, \"lxml\")\n",
    "soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1.  selector 제목, 링크 수집하기 (h3 ) \n",
    "header2 = soup2.select(\"div.left_cont> div.news_box > div.newsPost > div.assetText > a > h3\")\n",
    "print(len(header2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e2f3659d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3>배경훈 장관 후보자 인사청문 시작도 못하고 파행</h3>,\n",
       " <h3>SK브로드밴드 \"365일 설치 서비스 시행\"</h3>,\n",
       " <h3>111퍼센트, '슈퍼패스트 배치' 2기 모집 설명회 마련</h3>,\n",
       " <h3>티맥스소프트, '애플리케이션 현대화 전략' 제시</h3>,\n",
       " <h3>\"OT보안 중요성 커져···95%가 C레벨이 책임 \"</h3>,\n",
       " <h3>스트라타시스가 제시한 3D프린팅 산업 트렌드는?</h3>,\n",
       " <h3>해외로 가는 LG 스탠바이미 2, 이동식 스크린 시장 리더십 확장</h3>,\n",
       " <h3>\"생산성 높인다더니 업무시간 더 늘었다\"…AI 코딩 툴, 그래도 찾는 이유는?</h3>,\n",
       " <h3>포시에스, 日 파트너사 메트로와 현지 제조업 디지털 전환 시장 '정조준'</h3>,\n",
       " <h3>\"AI 개발 환경 강화\"…AWS, 세이지메이커 업그레이드</h3>,\n",
       " <h3>AI 번역기, 업무 필수 도구로…딥엘 \"韓 직장인 67.6% 사용\"</h3>,\n",
       " <h3>최휘영 문체부 장관 후보자 \"무거운 책임감, AI 시대 제반 분야 점검 노력\"</h3>,\n",
       " <h3>지멘스·SAP 대표 \"EU AI 규제, 혁신 가로막아…전면 재검토해야\"</h3>,\n",
       " <h3>쉐보레, 여름 무상점검 실시…전국 400여개 센터서 진행</h3>,\n",
       " <h3>KT스튜디오지니, 드라마박스와 차세대 숏폼 작가 발굴 '맞손'</h3>,\n",
       " <h3>\"아이폰17 시리즈, 9월 둘째 주 공개…9·10일 유력\"</h3>,\n",
       " <h3>한국엡손, 라벨프린터 패키지 한정판 3종 출시</h3>,\n",
       " <h3>국립문화유산연구원, 몽골 '하르보힝 발가스 유적' 공동조사</h3>,\n",
       " <h3>이탈리아 최대 방산업체 \"우크라이나 드론 공급 OK·현지 생산 NO\"</h3>,\n",
       " <h3>테슬라, 자동차에 자체 AI '그록' 탑재…차량용 기능은 없어</h3>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단수와 복수 구분!!\n",
    "# a = soup2.select(\"body > div.contentWrapper > div > div.left_cont > div.news_box > div:nth-child(1) > div.assetText > a > h3\")\n",
    "a = soup2.select(\"body > div.contentWrapper > div > div.left_cont > div.news_box > div > div.assetText > a > h3\")\n",
    "a\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2. selector  제목, 링크 수집하기 (h3와 anchor )\n",
    "aTag2 = soup2.select(\"div.left_cont > div.news_box > div.newsPost > div.assetText > a\")\n",
    "print(len(aTag2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bbb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 비어있는 리스트 제작 후, 컨텐츠와 href만 저장 \n",
    "# a tag 안에 있는 속성(href)을 저장한다 \n",
    "title_list2 = []\n",
    "url_list2 = []\n",
    "website = \"http://zdnet.co.kr\"\n",
    "\n",
    "for i in range(len(aTag2)):\n",
    "    title_list2.append(title[i].text) \n",
    "    url_list2.append(website + aTag2[i][\"href\"])\n",
    "\n",
    "print(url_list2)\n",
    "\n",
    "title_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b92e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11753df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 개수체크 20개씩\n",
    "print(len(header2),len(url_list2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebee005",
   "metadata": {},
   "source": [
    "# 반복하는 코드와 변화하는 부분을 찾아서 합치기\n",
    "- 10 페이지 수집하는 코드를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad754ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 19의 제목 개수 : 200, 링크 개수 : 200\n",
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "# page를 i는 1부터 시작할것이니깐  \n",
    "# 초기화가 되어버린다, 결과가 20개밖에 안나온다. \n",
    "# 반복하지 않는 코드를 밖에서 1번만 실행하게 해줘야한다 .(초기화)\n",
    "\n",
    "\n",
    "# 맨나중에 생각하기. title_list = []\n",
    "title_list = []\n",
    "# 맨나중에 생각하기. url_list = []\n",
    "url_list = []\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # 데이터 요청(request > response)\n",
    "    res = req.get(f\"https://zdnet.co.kr/news/?lstcode=0000&page={i}\")\n",
    "    # 적절한 data로 parsing (lxml parser이용해서  => html로 parsing)\n",
    "    soup= bs(res.text, \"lxml\")\n",
    "\n",
    "    #데이터 수집 (selector 이용해서 제목과 링크 수집)\n",
    "    title = soup.select(\"div.left_cont > div.news_box > div.newsPost > div.assetText > a > h3\")\n",
    "    aTag = soup.select(\"div.left_cont > div.news_box > div.newsPost > div.assetText > a\")\n",
    "\n",
    "    #데이터 가공 (원하는 데이터만 추출)\n",
    "    for i in range(len(title)):\n",
    "        title_list.append(title[i].text) \n",
    "        url_list.append(\"https://zdnet.co.kr/\" + aTag[i][\"href\"])\n",
    "\n",
    "print(f\"페이지 {i}의 제목 개수 : {len(title_list)}, 링크 개수 : {len(url_list)}\")\n",
    "print(type(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fef5a7",
   "metadata": {},
   "source": [
    "## df으로 시각화, 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e462a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517bca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스제목 200개 , url 200개 \n",
    "# data를 가지고있고, calumn이 없다.\n",
    "# 200개를 같이 묶어서 관리하겠다. \n",
    "# type이 동일하다 => list가 좋겠다. \n",
    "# title과 url은 type이 다르다 => dict가 좋겠다.\n",
    "\n",
    "# 표를 제작하기 위해서 컬럼과 데이터구조의 형태로 제작한다.\n",
    "data = { \"뉴스제목\" : title_list, \"뉴스링크\" : url_list} \n",
    "\n",
    "# df령식으로 변환시켜야한다. pandas 이용\n",
    "pd.DataFrame(data)\n",
    "\n",
    "# 변수에 담는다.\n",
    "news =pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6249a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장 (html)\n",
    "# 가져온다 form 내보낸다 to \n",
    "news.to_html(\"./뉴스제목.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장 (CSV, encoding= \"utf-8-sig\" (signature)) \n",
    "# 한글이 포함된 파일을 저장할 대는 encoding을 신경쓰자! \n",
    "news.to_csv(\"./뉴스제목.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700b990",
   "metadata": {},
   "source": [
    "# 클롤링 오전수업\n",
    "## 1. 속성수집(href, src, class, id 등등)\n",
    "- 속성을 가지고 있는 태그를 먼저 수집한다.\n",
    "- 속성을 접근하기 위해서는 [속성명]\n",
    "\n",
    "## 2.반복문 짜는 법\n",
    "- 핵심 : 문법으로 접근하지 말고, 완벽한 하나의 사이클을 만들자.\n",
    "- 반복할 코드와 반복하지 않을 코드를 분리한다.\n",
    "- 반복문으로 묶어주고 마지막에 i의 위치를 찾아준다.\n",
    "## 3. req로 여러페이지를 수집하기 위해서는 url의 변화에 집중하자.\n",
    "- 요청할 대 마다, 변하는 url로 재요청 해결이 가능\n",
    "## 4. 응답코드가 400번대 경우 해결책\n",
    "- 400번대 오류는 요청하는 나의 문재\n",
    "- 1) 잘못된 url을 요청했을 때\n",
    "- 2) 브라우저가 아님을 감지 당할때.\n",
    "- user-agnet코드를 요청할 때 같이 보낸다.\n",
    "\n",
    "## 5. 오후 실습\n",
    "- 가수, 노래 수집 (선택자)\n",
    "- 환율정보사이트 수집(수집불가) = res 200 but no data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
