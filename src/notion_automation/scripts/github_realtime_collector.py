"""
Task 4.2.1: GitHub MCP ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

Phase 4: GitHub ì‹œê°„ëŒ€ë³„ ì—°ë™ & ì •ëŸ‰í™”
ì‹¤ì‹œê°„ìœ¼ë¡œ GitHub í™œë™ì„ ìˆ˜ì§‘í•˜ì—¬ ì‹œê°„ëŒ€ë³„ë¡œ ë¶„ë¥˜í•˜ê³ 
3-Part Notion DBì— ìë™ ì—°ë™í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Author: AI Assistant (ì´ì–´ë°›ì•„ êµ¬í˜„)
Date: 2025-07-05
"""

import os
import sys
import json
import logging
from datetime import datetime, date, timedelta
from typing import Dict, List, Any, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
sys.path.insert(0, project_root)

from src.notion_automation.utils.logger import ThreePartLogger
from src.notion_automation.core.github_time_analyzer import GitHubTimeAnalyzer

logger = ThreePartLogger("github_realtime_collector")

class GitHubRealtimeCollector:
    """GitHub MCP ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° Notion ì—°ë™ ì‹œìŠ¤í…œ"""
    
    def __init__(self, owner: Optional[str] = None, repo: Optional[str] = None):
        """
        ì‹¤ì‹œê°„ GitHub ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            owner: GitHub ì €ì¥ì†Œ ì†Œìœ ì
            repo: GitHub ì €ì¥ì†Œ ì´ë¦„
        """
        self.owner = owner or os.getenv("GITHUB_OWNER", "user")
        self.repo = repo or os.getenv("GITHUB_REPO", "LG_DX_School")
        
        # GitHub ì‹œê°„ëŒ€ë³„ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.analyzer = GitHubTimeAnalyzer(owner=self.owner, repo=self.repo)
        
        # ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì„¤ì •
        self.collection_config = {
            "enable_real_github_api": False,  # ì‹¤ì œ GitHub API ì‚¬ìš© ì—¬ë¶€
            "use_simulation": True,           # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì‚¬ìš©
            "backup_to_local": True,         # ë¡œì»¬ ë°±ì—… í™œì„±í™”
            "auto_notion_sync": True,        # Notion ìë™ ë™ê¸°í™”
            "error_retry_count": 3,          # ì—ëŸ¬ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
            "api_timeout_seconds": 30        # API íƒ€ì„ì•„ì›ƒ ì‹œê°„
        }
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir = os.path.join(project_root, "data", "github_realtime")
        os.makedirs(self.backup_dir, exist_ok=True)
        
        logger.info("GitHub ì‹¤ì‹œê°„ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def collect_realtime_github_data(self, target_date: Optional[date] = None, 
                                   specific_timepart: Optional[str] = None) -> Dict[str, Any]:
        """
        ì‹¤ì‹œê°„ GitHub í™œë™ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            target_date: ìˆ˜ì§‘í•  ë‚ ì§œ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
            specific_timepart: íŠ¹ì • ì‹œê°„ëŒ€ë§Œ ìˆ˜ì§‘ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì‹œê°„ëŒ€ë³„ GitHub í™œë™ ë°ì´í„°
        """
        if target_date is None:
            target_date = datetime.now().date()
        
        logger.info(f"ì‹¤ì‹œê°„ GitHub ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {target_date}")
        
        try:
            # í˜„ì¬ ì‹œê°„ëŒ€ ìë™ ì‹ë³„
            current_timepart = self._get_current_timepart()
            
            # íŠ¹ì • ì‹œê°„ëŒ€ ì§€ì •ì´ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ëŒ€ ì‚¬ìš©
            if specific_timepart is None:
                timeparts_to_collect = [current_timepart]
                logger.info(f"í˜„ì¬ ì‹œê°„ëŒ€ ìë™ ì‹ë³„: {current_timepart}")
            else:
                timeparts_to_collect = [specific_timepart]
                logger.info(f"ì§€ì •ëœ ì‹œê°„ëŒ€ ìˆ˜ì§‘: {specific_timepart}")
            
            # ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜ì§‘
            collected_data = {}
            
            for timepart in timeparts_to_collect:
                logger.info(f"{timepart} ì‹œê°„ëŒ€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                
                # GitHub í™œë™ ìˆ˜ì§‘ (ì‹¤ì œ ë˜ëŠ” ì‹œë®¬ë ˆì´ì…˜)
                github_activities = self._collect_timepart_activities(target_date, timepart)
                
                # ë°ì´í„° ê²€ì¦ ë° ì •ì œ
                validated_data = self._validate_and_clean_data(github_activities)
                
                # ë¡œì»¬ ë°±ì—…
                if self.collection_config["backup_to_local"]:
                    self._backup_to_local(validated_data, target_date, timepart)
                
                collected_data[timepart] = validated_data
                
                logger.info(f"{timepart} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: "
                          f"{validated_data.get('commits', 0)}ê°œ ì»¤ë°‹, "
                          f"ìƒì‚°ì„± ì ìˆ˜ {validated_data.get('productive_score', 0)}ì ")
            
            # ìˆ˜ì§‘ ê²°ê³¼ ì¢…í•©
            collection_result = {
                "collection_date": target_date.isoformat(),
                "collection_time": datetime.now().isoformat(),
                "collected_timeparts": list(collected_data.keys()),
                "total_timeparts": len(collected_data),
                "data": collected_data,
                "collection_success": True,
                "collection_method": "simulation" if self.collection_config["use_simulation"] else "real_api",
                "notes": f"ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì™„ë£Œ - {len(collected_data)}ê°œ ì‹œê°„ëŒ€"
            }
            
            logger.info(f"ì‹¤ì‹œê°„ GitHub ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data)}ê°œ ì‹œê°„ëŒ€")
            return collection_result
            
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ GitHub ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return self._create_empty_collection_result(target_date, str(e))

    def _get_current_timepart(self) -> str:
        """í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ì‹œê°„ëŒ€ ë°˜í™˜"""
        current_hour = datetime.now().hour
        
        # ì‹œê°„ëŒ€ ë§¤í•‘
        if 9 <= current_hour < 12:
            return "ğŸŒ… ì˜¤ì „ìˆ˜ì—…"
        elif 13 <= current_hour < 17:
            return "ğŸŒ ì˜¤í›„ìˆ˜ì—…"
        elif 19 <= current_hour < 22:
            return "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ"
        else:
            # ìˆ˜ì—… ì‹œê°„ì´ ì•„ë‹Œ ê²½ìš° ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ëŒ€ ë°˜í™˜
            if current_hour < 9:
                return "ğŸŒ… ì˜¤ì „ìˆ˜ì—…"
            elif current_hour < 13:
                return "ğŸŒ… ì˜¤ì „ìˆ˜ì—…"
            elif current_hour < 19:
                return "ğŸŒ ì˜¤í›„ìˆ˜ì—…"
            else:
                return "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ"

    def _collect_timepart_activities(self, target_date: date, timepart: str) -> Dict[str, Any]:
        """íŠ¹ì • ì‹œê°„ëŒ€ì˜ GitHub í™œë™ ìˆ˜ì§‘"""
        try:
            if self.collection_config["enable_real_github_api"]:
                # ì‹¤ì œ GitHub API í˜¸ì¶œ (í–¥í›„ êµ¬í˜„)
                return self._collect_real_github_activities(target_date, timepart)
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
                return self._collect_simulated_activities(target_date, timepart)
                
        except Exception as e:
            logger.error(f"{timepart} í™œë™ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return self._create_empty_timepart_data(timepart)

    def _collect_real_github_activities(self, target_date: date, timepart: str) -> Dict[str, Any]:
        """ì‹¤ì œ GitHub MCPë¥¼ í†µí•œ í™œë™ ìˆ˜ì§‘ (í–¥í›„ êµ¬í˜„)"""
        # TODO: ì‹¤ì œ GitHub MCP ë„êµ¬ ì—°ë™
        logger.warning("ì‹¤ì œ GitHub MCP ì—°ë™ì€ ì¶”í›„ êµ¬í˜„ ì˜ˆì •")
        
        # í˜„ì¬ëŠ” ê¸°ì¡´ ë¶„ì„ê¸° í™œìš©
        return self.analyzer.get_time_part_activities(target_date, timepart)

    def _collect_simulated_activities(self, target_date: date, timepart: str) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ GitHub í™œë™ ìˆ˜ì§‘"""
        logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ {timepart} í™œë™ ìˆ˜ì§‘")
        
        # ê¸°ì¡´ GitHub ë¶„ì„ê¸° í™œìš©
        activities = self.analyzer.get_time_part_activities(target_date, timepart)
        
        # ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì •ë³´ ì¶”ê°€
        activities.update({
            "collection_method": "simulation",
            "collection_timestamp": datetime.now().isoformat(),
            "is_realtime": True,
            "data_source": "github_time_analyzer"
        })
        
        return activities

    def _validate_and_clean_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ["date", "time_part", "commits", "issues", "pull_requests"]
            for field in required_fields:
                if field not in raw_data:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    raw_data[field] = [] if field in ["commits", "issues", "pull_requests"] else ""
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            if not isinstance(raw_data.get("commits", []), list):
                raw_data["commits"] = []
            
            if not isinstance(raw_data.get("issues", []), list):
                raw_data["issues"] = []
            
            if not isinstance(raw_data.get("pull_requests", []), list):
                raw_data["pull_requests"] = []
            
            # ìƒì‚°ì„± ì ìˆ˜ ê²€ì¦
            if "productive_score" not in raw_data or not isinstance(raw_data["productive_score"], (int, float)):
                raw_data["productive_score"] = 0
            
            # ê²€ì¦ ì™„ë£Œ í‘œì‹œ
            raw_data["data_validated"] = True
            raw_data["validation_timestamp"] = datetime.now().isoformat()
            
            logger.info("ë°ì´í„° ê²€ì¦ ë° ì •ì œ ì™„ë£Œ")
            return raw_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return raw_data

    def _backup_to_local(self, data: Dict[str, Any], target_date: date, timepart: str):
        """ë¡œì»¬ íŒŒì¼ë¡œ ë°ì´í„° ë°±ì—…"""
        try:
            # íŒŒì¼ëª… ìƒì„±
            timepart_safe = timepart.replace("ğŸŒ…", "morning").replace("ğŸŒ", "afternoon").replace("ğŸŒ™", "evening")
            timepart_safe = timepart_safe.replace(" ", "_").replace("ììœ¨í•™ìŠµ", "study")
            
            filename = f"github_realtime_{timepart_safe}_{target_date.strftime('%Y%m%d')}.json"
            filepath = os.path.join(self.backup_dir, filename)
            
            # JSON í˜•íƒœë¡œ ì €ì¥
            backup_data = {
                "backup_info": {
                    "created_at": datetime.now().isoformat(),
                    "target_date": target_date.isoformat(),
                    "timepart": timepart,
                    "collector_version": "4.2.1"
                },
                "github_data": data
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë¡œì»¬ ë°±ì—… ì™„ë£Œ: {filepath}")
            
        except Exception as e:
            logger.error(f"ë¡œì»¬ ë°±ì—… ì‹¤íŒ¨: {str(e)}")

    def _create_empty_timepart_data(self, timepart: str) -> Dict[str, Any]:
        """ë¹ˆ ì‹œê°„ëŒ€ ë°ì´í„° ìƒì„±"""
        return {
            "date": datetime.now().date().isoformat(),
            "time_part": timepart,
            "commits": [],
            "issues": [],
            "pull_requests": [],
            "code_reviews": [],
            "productive_score": 0,
            "collection_method": "empty",
            "error": True,
            "data_source": "fallback"
        }

    def _create_empty_collection_result(self, target_date: date, error_message: str) -> Dict[str, Any]:
        """ë¹ˆ ìˆ˜ì§‘ ê²°ê³¼ ìƒì„±"""
        return {
            "collection_date": target_date.isoformat(),
            "collection_time": datetime.now().isoformat(),
            "collected_timeparts": [],
            "total_timeparts": 0,
            "data": {},
            "collection_success": False,
            "error_message": error_message,
            "collection_method": "failed"
        }

    def integrate_with_notion_3part(self, github_data: Dict[str, Any], 
                                  reflection_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        GitHub ë°ì´í„°ë¥¼ 3-Part Notion DBì— ìë™ í†µí•©
        
        Args:
            github_data: ìˆ˜ì§‘ëœ GitHub í™œë™ ë°ì´í„°
            reflection_data: ê¸°ì¡´ ë°˜ì„± ì…ë ¥ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Notion í†µí•© ê²°ê³¼
        """
        logger.info("GitHub ë°ì´í„° Notion 3-Part DB í†µí•© ì‹œì‘")
        
        try:
            # í†µí•©í•  ë°ì´í„° ì¤€ë¹„
            integrated_data = self._prepare_notion_integration_data(github_data, reflection_data)
            
            # Notion MCP ì—°ë™ (í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜)
            if self.collection_config["auto_notion_sync"]:
                notion_result = self._sync_to_notion_database(integrated_data)
            else:
                notion_result = {"status": "disabled", "message": "Notion ë™ê¸°í™”ê°€ ë¹„í™œì„±í™”ë¨"}
            
            integration_result = {
                "integration_timestamp": datetime.now().isoformat(),
                "github_data_processed": True,
                "notion_sync_result": notion_result,
                "integrated_timeparts": list(github_data.get("data", {}).keys()),
                "success": True
            }
            
            logger.info("GitHub-Notion í†µí•© ì™„ë£Œ")
            return integration_result
            
        except Exception as e:
            logger.error(f"GitHub-Notion í†µí•© ì‹¤íŒ¨: {str(e)}")
            return {
                "integration_timestamp": datetime.now().isoformat(),
                "success": False,
                "error": str(e)
            }

    def _prepare_notion_integration_data(self, github_data: Dict[str, Any], 
                                       reflection_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Notion í†µí•©ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        integrated_data = {}
        
        # GitHub ë°ì´í„° ì²˜ë¦¬
        for timepart, activities in github_data.get("data", {}).items():
            integrated_entry = {
                "time_part": timepart,
                "date": activities.get("date"),
                "github_commit_count": len(activities.get("commits", [])),
                "github_issue_count": len(activities.get("issues", [])),
                "github_pr_count": len(activities.get("pull_requests", [])),
                "github_productivity_score": activities.get("productive_score", 0),
                "github_activities_summary": self._create_activities_summary(activities),
                "data_source": "github_realtime_collector"
            }
            
            # ê¸°ì¡´ ë°˜ì„± ë°ì´í„°ì™€ ë³‘í•© (ìˆëŠ” ê²½ìš°)
            if reflection_data and timepart in reflection_data:
                integrated_entry.update(reflection_data[timepart])
            
            integrated_data[timepart] = integrated_entry
        
        return integrated_data

    def _create_activities_summary(self, activities: Dict[str, Any]) -> str:
        """GitHub í™œë™ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±"""
        commits = activities.get("commits", [])
        issues = activities.get("issues", [])
        prs = activities.get("pull_requests", [])
        
        summary_parts = []
        
        if commits:
            summary_parts.append(f"ì»¤ë°‹ {len(commits)}ê°œ")
            # ì£¼ìš” ì»¤ë°‹ ë©”ì‹œì§€ í¬í•¨
            if len(commits) > 0:
                main_commit = commits[0].get("message", "")[:50]
                summary_parts.append(f"(ì£¼ìš”: {main_commit}...)")
        
        if issues:
            summary_parts.append(f"ì´ìŠˆ {len(issues)}ê°œ")
        
        if prs:
            summary_parts.append(f"PR {len(prs)}ê°œ")
        
        return ", ".join(summary_parts) if summary_parts else "í™œë™ ì—†ìŒ"

    def _sync_to_notion_database(self, integrated_data: Dict[str, Any]) -> Dict[str, Any]:
        """Notion ë°ì´í„°ë² ì´ìŠ¤ì— ë™ê¸°í™” (í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info("Notion DB ë™ê¸°í™” ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
        
        # TODO: ì‹¤ì œ mcp_notion ë„êµ¬ ì—°ë™
        sync_results = []
        
        for timepart, entry_data in integrated_data.items():
            # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
            sync_result = {
                "timepart": timepart,
                "notion_page_id": f"mock_page_id_{timepart}_{datetime.now().timestamp()}",
                "sync_status": "success",
                "sync_method": "simulation",
                "data_fields_updated": len(entry_data)
            }
            sync_results.append(sync_result)
        
        return {
            "sync_timestamp": datetime.now().isoformat(),
            "total_entries": len(sync_results),
            "successful_syncs": len(sync_results),
            "failed_syncs": 0,
            "sync_details": sync_results,
            "method": "simulation"
        }

    def handle_api_errors_and_retry(self, operation_func, max_retries: Optional[int] = None) -> Any:
        """
        API ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        
        Args:
            operation_func: ì‹¤í–‰í•  í•¨ìˆ˜
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼
        """
        if max_retries is None:
            max_retries = self.collection_config["error_retry_count"]
        
        last_error: Optional[Exception] = None
        
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries + 1}")
                result = operation_func()
                logger.info("API í˜¸ì¶œ ì„±ê³µ")
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {str(e)}")
                
                if attempt < max_retries:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
                    wait_time = 2 ** attempt
                    logger.info(f"{wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    import time
                    time.sleep(wait_time)
                else:
                    logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë§ˆì§€ë§‰ ì—ëŸ¬: {str(e)}")
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        if last_error is not None:
            raise last_error
        else:
            raise Exception("ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹¤íŒ¨")

    def get_collection_status(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ê¸° ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            "collector_info": {
                "version": "4.2.1",
                "owner": self.owner,
                "repo": self.repo,
                "status": "active"
            },
            "configuration": self.collection_config,
            "backup_directory": self.backup_dir,
            "current_timepart": self._get_current_timepart(),
            "analyzer_available": self.analyzer is not None
        }


def test_realtime_collector():
    """GitHub ì‹¤ì‹œê°„ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Task 4.2.1: GitHub MCP ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = GitHubRealtimeCollector(owner="test_user", repo="LG_DX_School")
    
    # ìƒíƒœ í™•ì¸
    status = collector.get_collection_status()
    print(f"ğŸ“Š ìˆ˜ì§‘ê¸° ìƒíƒœ:")
    print(f"   ë²„ì „: {status['collector_info']['version']}")
    print(f"   ëŒ€ìƒ ì €ì¥ì†Œ: {status['collector_info']['owner']}/{status['collector_info']['repo']}")
    print(f"   í˜„ì¬ ì‹œê°„ëŒ€: {status['current_timepart']}")
    print(f"   ë°±ì—… ë””ë ‰í† ë¦¬: {status['backup_directory']}")
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”„ ì‹¤ì‹œê°„ GitHub ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸:")
    test_date = datetime.now().date()
    
    # í˜„ì¬ ì‹œê°„ëŒ€ ìë™ ìˆ˜ì§‘
    collection_result = collector.collect_realtime_github_data(test_date)
    
    if collection_result["collection_success"]:
        print(f"âœ… ìˆ˜ì§‘ ì„±ê³µ!")
        print(f"   ìˆ˜ì§‘ ë‚ ì§œ: {collection_result['collection_date']}")
        print(f"   ìˆ˜ì§‘ ì‹œê°„ëŒ€: {collection_result['collected_timeparts']}")
        print(f"   ìˆ˜ì§‘ ë°©ë²•: {collection_result['collection_method']}")
        
        # ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½
        for timepart, data in collection_result["data"].items():
            print(f"\nğŸ“‹ {timepart} ìˆ˜ì§‘ ê²°ê³¼:")
            print(f"   ì»¤ë°‹: {len(data.get('commits', []))}ê°œ")
            print(f"   ì´ìŠˆ: {len(data.get('issues', []))}ê°œ")
            print(f"   PR: {len(data.get('pull_requests', []))}ê°œ")
            print(f"   ìƒì‚°ì„± ì ìˆ˜: {data.get('productive_score', 0)}ì ")
    else:
        print(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {collection_result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    # Notion í†µí•© í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”— Notion 3-Part DB í†µí•© í…ŒìŠ¤íŠ¸:")
    integration_result = collector.integrate_with_notion_3part(collection_result)
    
    if integration_result["success"]:
        print(f"âœ… í†µí•© ì„±ê³µ!")
        print(f"   í†µí•© ì‹œê°„ëŒ€: {integration_result['integrated_timeparts']}")
        print(f"   Notion ë™ê¸°í™”: {integration_result['notion_sync_result']['method']}")
    else:
        print(f"âŒ í†µí•© ì‹¤íŒ¨: {integration_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    print(f"\nğŸ‰ Task 4.2.1 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    return True


if __name__ == "__main__":
    test_realtime_collector()
