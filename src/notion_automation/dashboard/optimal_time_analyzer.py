"""
ê°œì¸ë³„ ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ë¶„ì„ ë° ì¶”ì²œ ì‹œìŠ¤í…œ
3-Part ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì¸í™”ëœ í•™ìŠµ ì „ëµ ì œê³µ
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.join(os.path.dirname(__file__), '..', '..', '..')
sys.path.append(project_root)

from src.notion_automation.utils.logger import ThreePartLogger

class OptimalTimeAnalyzer:
    """ê°œì¸ë³„ ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = ThreePartLogger()
        self.data_dir = os.path.join(project_root, 'data')
        
        # ë¶„ì„ ì°¨ì› ì •ì˜
        self.analysis_dimensions = {
            "understanding": "ì´í•´ë„",
            "github_activity": "GitHub í™œë™",
            "condition": "ì»¨ë””ì…˜",
            "concentration": "ì§‘ì¤‘ë„",
            "learning_time": "í•™ìŠµì‹œê°„",
            "overall_score": "ì¢…í•©ì ìˆ˜"
        }
        
        # í•™ìŠµ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        self.learning_type_weights = {
            "ì´ë¡ í•™ìŠµ": {
                "understanding": 0.4,
                "concentration": 0.3,
                "condition": 0.2,
                "github_activity": 0.05,
                "learning_time": 0.05
            },
            "ì‹¤ìŠµì½”ë”©": {
                "github_activity": 0.35,
                "concentration": 0.25,
                "understanding": 0.2,
                "condition": 0.15,
                "learning_time": 0.05
            },
            "í”„ë¡œì íŠ¸": {
                "github_activity": 0.3,
                "concentration": 0.25,
                "condition": 0.2,
                "understanding": 0.15,
                "learning_time": 0.1
            },
            "ë³µìŠµì •ë¦¬": {
                "understanding": 0.35,
                "concentration": 0.3,
                "condition": 0.2,
                "learning_time": 0.1,
                "github_activity": 0.05
            }
        }
    
    def load_comprehensive_data(self, days: int = 14) -> Dict[str, Dict[str, Any]]:
        """
        ì¢…í•©ì ì¸ 3-Part ë°ì´í„° ë¡œë“œ
        
        Args:
            days: ë¶„ì„í•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 14ì¼)
            
        Returns:
            ë‚ ì§œë³„, ì‹œê°„ëŒ€ë³„ ì¢…í•© ë°ì´í„°
        """
        try:
            comprehensive_data = {}
            
            for day_offset in range(days):
                date = datetime.now() - timedelta(days=day_offset)
                date_str = date.strftime("%Y-%m-%d")
                weekday = date.strftime("%A")
                
                day_data = {
                    "date": date_str,
                    "weekday": weekday,
                    "timeparts": {}
                }
                
                # ê° ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜ì§‘
                timepart_configs = {
                    "ğŸŒ… ì˜¤ì „ìˆ˜ì—…": ("morning_reflections", f"morning_reflection_{date.strftime('%Y%m%d')}.json"),
                    "ğŸŒ ì˜¤í›„ìˆ˜ì—…": ("afternoon_reflections", f"afternoon_reflection_{date.strftime('%Y%m%d')}.json"),
                    "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ": ("evening_reflections", f"evening_reflection_{date.strftime('%Y%m%d')}.json")
                }
                
                for timepart, (folder, filename) in timepart_configs.items():
                    file_path = os.path.join(self.data_dir, folder, filename)
                    
                    timepart_data = {
                        "understanding": 0,
                        "concentration": 0,
                        "condition": 0,
                        "github_activity": 0,
                        "learning_time": 0,
                        "overall_score": 0,
                        "has_data": False
                    }
                    
                    if os.path.exists(file_path):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            timepart_data = self._extract_timepart_metrics(data, timepart)
                            timepart_data["has_data"] = True
                    
                    day_data["timeparts"][timepart] = timepart_data
                
                comprehensive_data[date_str] = day_data
            
            self.logger.info(f"ì¢…í•© ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(comprehensive_data)}ì¼ ë°ì´í„°")
            return comprehensive_data
            
        except Exception as e:
            self.logger.log_error(e, "ì¢…í•© ë°ì´í„° ë¡œë“œ")
            return {}
    
    def _extract_timepart_metrics(self, data: Dict[str, Any], timepart: str) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ ë°ì´í„°ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        try:
            # ì´í•´ë„
            understanding = data.get('í•™ìŠµì´í•´ë„', data.get('ì´í•´ë„', 0))
            
            # ì§‘ì¤‘ë„ 
            concentration = data.get('ì§‘ì¤‘ë„', data.get('ê³„íšë‹¬ì„±ë„', 0))
            
            # ì»¨ë””ì…˜ (ìˆ«ìë¡œ ë³€í™˜)
            condition_text = data.get('ì»¨ë””ì…˜', 'ë³´í†µ')
            condition = {"ì¢‹ìŒ": 8, "ë³´í†µ": 5, "ë‚˜ì¨": 2}.get(condition_text, 5)
            
            # GitHub í™œë™
            github_data = data.get('github_data', {})
            github_activity = (
                github_data.get('commits', 0) * 2 +
                github_data.get('issues', 0) * 1.5 +
                github_data.get('pull_requests', 0) * 3
            )
            
            # í•™ìŠµ ì‹œê°„
            learning_time = data.get('í•™ìŠµì‹œê°„', data.get('ì‹¤ì œí•™ìŠµì‹œê°„', 0))
            
            # ì¢…í•© ì ìˆ˜
            overall_score = data.get('ì´ì ', 0)
            
            return {
                "understanding": understanding,
                "concentration": concentration,
                "condition": condition,
                "github_activity": min(github_activity, 20),  # ìµœëŒ€ 20ì ìœ¼ë¡œ ì œí•œ
                "learning_time": learning_time,
                "overall_score": overall_score
            }
            
        except Exception as e:
            self.logger.log_error(e, f"ë©”íŠ¸ë¦­ ì¶”ì¶œ ({timepart})")
            return {
                "understanding": 0,
                "concentration": 0,
                "condition": 0,
                "github_activity": 0,
                "learning_time": 0,
                "overall_score": 0
            }
    
    def identify_optimal_learning_times(self, days: int = 14) -> Dict[str, Any]:
        """
        ê°œì¸ì˜ 3-Part ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ì‹ë³„
        
        Args:
            days: ë¶„ì„í•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 14ì¼)
            
        Returns:
            ìµœì  ì‹œê°„ëŒ€ ë¶„ì„ ê²°ê³¼
        """
        try:
            # ì¢…í•© ë°ì´í„° ë¡œë“œ
            comprehensive_data = self.load_comprehensive_data(days)
            
            # ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ì§‘ê³„
            timepart_performance = {}
            for timepart in ["ğŸŒ… ì˜¤ì „ìˆ˜ì—…", "ğŸŒ ì˜¤í›„ìˆ˜ì—…", "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ"]:
                timepart_performance[timepart] = self._aggregate_timepart_performance(
                    comprehensive_data, timepart
                )
            
            # ë‹¤ì°¨ì› ë¶„ì„
            dimensional_analysis = {}
            for dimension, korean_name in self.analysis_dimensions.items():
                best_timepart = self._find_best_timepart_for_dimension(
                    timepart_performance, dimension
                )
                dimensional_analysis[korean_name] = best_timepart
            
            # í•™ìŠµ ìœ í˜•ë³„ ìµœì  ì‹œê°„ëŒ€
            learning_type_optimal = {}
            for learning_type, weights in self.learning_type_weights.items():
                optimal_timepart = self._calculate_weighted_optimal_time(
                    timepart_performance, weights
                )
                learning_type_optimal[learning_type] = optimal_timepart
            
            # ì¼ê´€ì„± ë¶„ì„
            consistency_analysis = self._analyze_consistency(comprehensive_data)
            
            # ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„
            weekday_patterns = self._analyze_weekday_patterns(comprehensive_data)
            
            # ì¢…í•© ì¶”ì²œ
            overall_recommendation = self._generate_overall_recommendation(
                timepart_performance, dimensional_analysis, learning_type_optimal
            )
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°
            analysis_result = {
                "analysis_type": "optimal_time_identification",
                "title": f"ğŸ¯ ê°œì¸ë³„ ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ë¶„ì„ (ìµœê·¼ {days}ì¼)",
                "analysis_period": f"{days}ì¼ê°„",
                "timepart_performance": timepart_performance,
                "dimensional_analysis": dimensional_analysis,
                "learning_type_optimal": learning_type_optimal,
                "consistency_analysis": consistency_analysis,
                "weekday_patterns": weekday_patterns,
                "overall_recommendation": overall_recommendation,
                "personalized_recommendations": self._generate_personalized_recommendations(
                    timepart_performance, dimensional_analysis, learning_type_optimal
                ),
                "created_at": datetime.now().isoformat()
            }
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            output_path = os.path.join(self.data_dir, f"optimal_time_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ìµœì  ì‹œê°„ëŒ€ ë¶„ì„ ì™„ë£Œ: {output_path}")
            return analysis_result
            
        except Exception as e:
            self.logger.log_error(e, "ìµœì  ì‹œê°„ëŒ€ ë¶„ì„")
            return {}
    
    def _aggregate_timepart_performance(self, data: Dict[str, Dict], timepart: str) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ì§‘ê³„"""
        try:
            metrics = {
                "understanding": [],
                "concentration": [],
                "condition": [],
                "github_activity": [],
                "learning_time": [],
                "overall_score": []
            }
            
            valid_days = 0
            
            for date_str, day_data in data.items():
                timepart_data = day_data["timeparts"].get(timepart, {})
                
                if timepart_data.get("has_data", False):
                    valid_days += 1
                    for metric in metrics.keys():
                        value = timepart_data.get(metric, 0)
                        metrics[metric].append(value)
            
            # í†µê³„ ê³„ì‚°
            aggregated = {
                "valid_days": valid_days,
                "activity_rate": (valid_days / len(data)) * 100 if data else 0
            }
            
            for metric, values in metrics.items():
                if values:
                    aggregated[metric] = {
                        "average": sum(values) / len(values),
                        "max": max(values),
                        "min": min(values),
                        "total": sum(values),
                        "consistency": self._calculate_consistency_score(values)
                    }
                else:
                    aggregated[metric] = {
                        "average": 0, "max": 0, "min": 0, "total": 0, "consistency": 0
                    }
            
            return aggregated
            
        except Exception as e:
            self.logger.log_error(e, f"ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ì§‘ê³„ ({timepart})")
            return {}
    
    def _calculate_consistency_score(self, values: List[float]) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë‚®ì€ ë³€ë™ì„± = ë†’ì€ ì¼ê´€ì„±)"""
        if len(values) <= 1:
            return 0
        
        mean_val = sum(values) / len(values)
        variance = sum([(v - mean_val) ** 2 for v in values]) / len(values)
        std_dev = variance ** 0.5
        
        # í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±ì´ ë†’ìŒ (0-10 ìŠ¤ì¼€ì¼)
        consistency = max(0, 10 - std_dev)
        return round(consistency, 1)
    
    def _find_best_timepart_for_dimension(self, timepart_performance: Dict, dimension: str) -> Dict[str, Any]:
        """íŠ¹ì • ì°¨ì›ì—ì„œ ìµœê³  ì„±ê³¼ ì‹œê°„ëŒ€ ì°¾ê¸°"""
        try:
            timepart_scores = {}
            
            for timepart, performance in timepart_performance.items():
                dimension_data = performance.get(dimension, {})
                score = dimension_data.get("average", 0)
                timepart_scores[timepart] = score
            
            if timepart_scores:
                best_timepart = max(timepart_scores.keys(), key=lambda x: timepart_scores[x])
                best_score = timepart_scores[best_timepart]
                
                return {
                    "best_timepart": best_timepart,
                    "score": round(best_score, 1),
                    "all_scores": {tp: round(score, 1) for tp, score in timepart_scores.items()}
                }
            
            return {"best_timepart": None, "score": 0, "all_scores": {}}
            
        except Exception as e:
            self.logger.log_error(e, f"ìµœê³  ì„±ê³¼ ì‹œê°„ëŒ€ ì°¾ê¸° ({dimension})")
            return {"best_timepart": None, "score": 0, "all_scores": {}}
    
    def _calculate_weighted_optimal_time(self, timepart_performance: Dict, weights: Dict[str, float]) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìµœì  ì‹œê°„ëŒ€ ê³„ì‚°"""
        try:
            timepart_weighted_scores = {}
            
            for timepart, performance in timepart_performance.items():
                weighted_score = 0
                
                for dimension, weight in weights.items():
                    dimension_score = performance.get(dimension, {}).get("average", 0)
                    weighted_score += dimension_score * weight
                
                timepart_weighted_scores[timepart] = weighted_score
            
            if timepart_weighted_scores:
                optimal_timepart = max(timepart_weighted_scores.keys(), 
                                     key=lambda x: timepart_weighted_scores[x])
                optimal_score = timepart_weighted_scores[optimal_timepart]
                
                return {
                    "optimal_timepart": optimal_timepart,
                    "weighted_score": round(optimal_score, 1),
                    "all_weighted_scores": {tp: round(score, 1) for tp, score in timepart_weighted_scores.items()}
                }
            
            return {"optimal_timepart": None, "weighted_score": 0, "all_weighted_scores": {}}
            
        except Exception as e:
            self.logger.log_error(e, "ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìµœì  ì‹œê°„ëŒ€ ê³„ì‚°")
            return {"optimal_timepart": None, "weighted_score": 0, "all_weighted_scores": {}}
    
    def _analyze_consistency(self, data: Dict[str, Dict]) -> Dict[str, Dict]:
        """ì¼ê´€ì„± ë¶„ì„"""
        consistency_analysis = {}
        
        try:
            for timepart in ["ğŸŒ… ì˜¤ì „ìˆ˜ì—…", "ğŸŒ ì˜¤í›„ìˆ˜ì—…", "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ"]:
                daily_scores = []
                
                for date_str, day_data in data.items():
                    timepart_data = day_data["timeparts"].get(timepart, {})
                    if timepart_data.get("has_data", False):
                        score = timepart_data.get("overall_score", 0)
                        daily_scores.append(score)
                
                if daily_scores:
                    consistency_score = self._calculate_consistency_score(daily_scores)
                    avg_score = sum(daily_scores) / len(daily_scores)
                    
                    consistency_analysis[timepart] = {
                        "consistency_score": consistency_score,
                        "average_performance": round(avg_score, 1),
                        "data_points": len(daily_scores),
                        "reliability": "ë†’ìŒ" if consistency_score >= 7 else "ë³´í†µ" if consistency_score >= 4 else "ë‚®ìŒ"
                    }
                
        except Exception as e:
            self.logger.log_error(e, "ì¼ê´€ì„± ë¶„ì„")
        
        return consistency_analysis
    
    def _analyze_weekday_patterns(self, data: Dict[str, Dict]) -> Dict[str, Any]:
        """ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„"""
        weekday_patterns = {}
        
        try:
            weekday_performance = {}
            
            for date_str, day_data in data.items():
                weekday = day_data["weekday"]
                
                if weekday not in weekday_performance:
                    weekday_performance[weekday] = {
                        "ğŸŒ… ì˜¤ì „ìˆ˜ì—…": [],
                        "ğŸŒ ì˜¤í›„ìˆ˜ì—…": [],
                        "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ": []
                    }
                
                for timepart in ["ğŸŒ… ì˜¤ì „ìˆ˜ì—…", "ğŸŒ ì˜¤í›„ìˆ˜ì—…", "ğŸŒ™ ì €ë…ììœ¨í•™ìŠµ"]:
                    timepart_data = day_data["timeparts"].get(timepart, {})
                    if timepart_data.get("has_data", False):
                        score = timepart_data.get("overall_score", 0)
                        weekday_performance[weekday][timepart].append(score)
            
            # ìš”ì¼ë³„ ìµœê³  ì‹œê°„ëŒ€ ì°¾ê¸°
            for weekday, timepart_scores in weekday_performance.items():
                best_timepart = None
                best_avg = 0
                
                for timepart, scores in timepart_scores.items():
                    if scores:
                        avg_score = sum(scores) / len(scores)
                        if avg_score > best_avg:
                            best_avg = avg_score
                            best_timepart = timepart
                
                weekday_patterns[weekday] = {
                    "best_timepart": best_timepart,
                    "best_score": round(best_avg, 1)
                }
            
        except Exception as e:
            self.logger.log_error(e, "ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„")
        
        return weekday_patterns
    
    def _generate_overall_recommendation(self, timepart_performance: Dict, dimensional_analysis: Dict, learning_type_optimal: Dict) -> str:
        """ì¢…í•© ì¶”ì²œì‚¬í•­ ìƒì„±"""
        try:
            # ê°€ì¥ ë§ì´ ì¶”ì²œë˜ëŠ” ì‹œê°„ëŒ€ ì°¾ê¸°
            timepart_mentions = {}
            
            # ì°¨ì›ë³„ ë¶„ì„ì—ì„œ ì¶”ì²œ ì§‘ê³„
            for dimension, result in dimensional_analysis.items():
                best_timepart = result.get("best_timepart")
                if best_timepart:
                    timepart_mentions[best_timepart] = timepart_mentions.get(best_timepart, 0) + 1
            
            # í•™ìŠµ ìœ í˜•ë³„ ë¶„ì„ì—ì„œ ì¶”ì²œ ì§‘ê³„
            for learning_type, result in learning_type_optimal.items():
                optimal_timepart = result.get("optimal_timepart")
                if optimal_timepart:
                    timepart_mentions[optimal_timepart] = timepart_mentions.get(optimal_timepart, 0) + 2  # í•™ìŠµ ìœ í˜•ì€ ê°€ì¤‘ì¹˜ 2ë°°
            
            if timepart_mentions:
                most_recommended = max(timepart_mentions.keys(), key=lambda x: timepart_mentions[x])
                recommendation_count = timepart_mentions[most_recommended]
                
                return f"{most_recommended}ì´ ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì‹œê°„ëŒ€ì…ë‹ˆë‹¤. ({recommendation_count}ê°œ ì˜ì—­ì—ì„œ ìµœì )"
            else:
                return "ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ëª…í™•í•œ ì¶”ì²œì´ ì–´ë µìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„° ì¶•ì  í›„ ì¬ë¶„ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
                
        except Exception as e:
            self.logger.log_error(e, "ì¢…í•© ì¶”ì²œì‚¬í•­ ìƒì„±")
            return "ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_personalized_recommendations(self, timepart_performance: Dict, dimensional_analysis: Dict, learning_type_optimal: Dict) -> List[str]:
        """ê°œì¸í™”ëœ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            # í•™ìŠµ ìœ í˜•ë³„ ì¶”ì²œ
            for learning_type, result in learning_type_optimal.items():
                optimal_timepart = result.get("optimal_timepart")
                if optimal_timepart:
                    recommendations.append(f"{learning_type}: {optimal_timepart}ì—ì„œ ê°€ì¥ íš¨ê³¼ì ")
            
            # íŠ¹ë³„í•œ ê°•ì  ì‹œê°„ëŒ€ ì¶”ì²œ
            for dimension, result in dimensional_analysis.items():
                best_timepart = result.get("best_timepart")
                score = result.get("score", 0)
                
                if score >= 8:  # ë†’ì€ ì ìˆ˜ì¸ ê²½ìš°ë§Œ
                    recommendations.append(f"{dimension} íŠ¹í™”: {best_timepart}ì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ ({score}ì )")
            
            # ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ ì§€ì 
            weakest_areas = []
            for timepart, performance in timepart_performance.items():
                activity_rate = performance.get("activity_rate", 0)
                if activity_rate < 50:  # í™œë™ë¥ ì´ 50% ë¯¸ë§Œ
                    weakest_areas.append(timepart)
            
            if weakest_areas:
                recommendations.append(f"í™œë™ ë¶€ì¡± ì‹œê°„ëŒ€: {', '.join(weakest_areas)} - ì •ê¸°ì ì¸ í•™ìŠµ ìŠµê´€ êµ¬ì¶• í•„ìš”")
            
            # ê· í˜•ì ì¸ ë°œì „ ì¶”ì²œ
            timepart_averages = {}
            for timepart, performance in timepart_performance.items():
                avg_overall = performance.get("overall_score", {}).get("average", 0)
                timepart_averages[timepart] = avg_overall
            
            if timepart_averages:
                best_timepart = max(timepart_averages.keys(), key=lambda x: timepart_averages[x])
                worst_timepart = min(timepart_averages.keys(), key=lambda x: timepart_averages[x])
                
                gap = timepart_averages[best_timepart] - timepart_averages[worst_timepart]
                if gap > 20:  # í° ê²©ì°¨ê°€ ìˆëŠ” ê²½ìš°
                    recommendations.append(f"ì‹œê°„ëŒ€ë³„ ê²©ì°¨ í¼: {worst_timepart} ì‹œê°„ëŒ€ ì§‘ì¤‘ ê°œì„  í•„ìš”")
            
        except Exception as e:
            self.logger.log_error(e, "ê°œì¸í™” ì¶”ì²œì‚¬í•­ ìƒì„±")
            recommendations.append("ê°œì¸í™” ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        return recommendations


def test_optimal_time_analyzer():
    """OptimalTimeAnalyzer í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ¯ ê°œì¸ë³„ ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    analyzer = OptimalTimeAnalyzer()
    
    # ìµœì  ì‹œê°„ëŒ€ ë¶„ì„ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ë¶„ì„ ì¤‘...")
    analysis_result = analyzer.identify_optimal_learning_times(days=14)
    
    if analysis_result:
        print("âœ… ìµœì  ì‹œê°„ëŒ€ ë¶„ì„ ì„±ê³µ!")
        print(f"ğŸ“Š ë¶„ì„ ê¸°ê°„: {analysis_result.get('analysis_period', 'N/A')}")
        
        # ì¢…í•© ì¶”ì²œ ì¶œë ¥
        overall_rec = analysis_result.get('overall_recommendation', '')
        if overall_rec:
            print(f"ğŸ† ì¢…í•© ì¶”ì²œ: {overall_rec}")
        
        # í•™ìŠµ ìœ í˜•ë³„ ìµœì  ì‹œê°„ëŒ€ ì¶œë ¥
        learning_optimal = analysis_result.get('learning_type_optimal', {})
        print(f"\nğŸ“š í•™ìŠµ ìœ í˜•ë³„ ìµœì  ì‹œê°„ëŒ€:")
        for learning_type, result in learning_optimal.items():
            optimal_timepart = result.get('optimal_timepart', 'N/A')
            score = result.get('weighted_score', 0)
            print(f"  {learning_type}: {optimal_timepart} ({score}ì )")
        
        # ê°œì¸í™” ì¶”ì²œì‚¬í•­ ê°œìˆ˜ ì¶œë ¥
        personal_recs = analysis_result.get('personalized_recommendations', [])
        print(f"\nğŸ’¡ ê°œì¸í™” ì¶”ì²œì‚¬í•­: {len(personal_recs)}ê°œ")
        
    else:
        print("âŒ ìµœì  ì‹œê°„ëŒ€ ë¶„ì„ ì‹¤íŒ¨")
    
    print("\nâœ… ê°œì¸ë³„ ìµœì  í•™ìŠµ ì‹œê°„ëŒ€ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_optimal_time_analyzer()
