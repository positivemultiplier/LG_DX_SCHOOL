{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fe81e4",
   "metadata": {},
   "source": [
    "# 뉴스데이터 수집\n",
    "- 뉴스의 타이틀만 수집\n",
    "- 선택자 분석법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 제목을 가져오는 함수 \n",
    "#  습관 1 : 검증후 변수에 넣기!!\n",
    "res = req.get(\"https://zdnet.co.kr/news/?lstcode=0000&page=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "561d89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. bs으로 HTML 데이터 변환\n",
    "soup = bs(res.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. html데이터에서 데이터 수집\n",
    "# 습관 2 : 개발자툴을 확인하고, 개발자도구에서 다시 영역확인하기.\n",
    "\n",
    "# 대화 : html 데이터에서 h3태그만 수집해\n",
    "## soup.find_all(\"h3\")\n",
    "## soup.select(\"h3\")\n",
    "\n",
    "# selector를 이용해서 h3 태그를 가져와야한다. 계층선택자 부모자식을 활용해서 span그룹을 제외한 data를 가져온다. \n",
    "#soup.select(\"a>h3\")\n",
    "\n",
    "# 부모도 id or class가 있을때까지 계층선택자를 진행하라\n",
    "# 클래스나 id 있는경우 복사 붙여넣기로 가져오기 !!! 오타조심 \n",
    "## soup.select(\"div.assetText>*>h3\")\n",
    "\n",
    "\n",
    "# 문제점 : 선택자의 범위가 넓어서 불필요한 데이터가 포함\n",
    "# 해결책 : 최대한 선택자의 범위를 좁히자.\n",
    "# 개념 : 내가 선택한 요소에 id, class 구분자가 없다면 부모태그를 활용하자. \n",
    "# 주의점 : 부모가 id, class를 가질 때 까지 좁힌다 =? 확률을 올리기 위해서 \n",
    "## soup.select(\"div.assetText>a>h3\")\n",
    "\n",
    "# 수집한 데이터 변수에 담기\n",
    "title = soup.select(\"div.assetText > a > h3\")\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e31fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 현재 수집된 요소(태그+속성+컨텐츠) 에서 => 필요한 제목만 추출(컨텐츠)\n",
    "\n",
    "title[0].text\n",
    "\n",
    "\n",
    "for i in range(len(title)):\n",
    "# title[i]를 넣게 된다면 tag와 content가 모두 출력된다. 따라서 title[i].text를 사용해서 content만 추출한다.\n",
    "    print(type(title[i].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ad212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 반목문을 통해서 모든 결과를 출력\n",
    "# len의 활용법 : for문의 반복횟수 사용, 데이터의 유무검증(if)\n",
    "# 비어있는 리스트에 글자만 저장할거야.\n",
    "title_list = []\n",
    "# 반복문을 통해서 title의 개수만큼 반복\n",
    "for i in range(len(title)):\n",
    "   # title[i]를 넣게 된다면 tag와 content가 모두 출력된다. 따라서 title[i].text를 사용해서 content만 추출한다.\n",
    "   title_list.append(title[i].text) \n",
    "\n",
    "title_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d79d5",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e38a5e",
   "metadata": {},
   "source": [
    "# 크롤링 1회차 정리\n",
    "\n",
    "- 1. 크롤링 : 웹상에 존재한느 데이터를 컴퓨터가 수집하는 기술\n",
    " - 컴퓨터와 대화를 한다고 생각\n",
    " - 사람이 하는 행위를 세분화 해서 명령을 내린다.\n",
    "- 2. requests 라이브러리\n",
    "  - 브라우저를 대신하는 라이브러리\n",
    "  - 파이썬에서 데이터를 요청하는 라이브러리\n",
    "  - req.get(\"url\")\n",
    "  - 핵심 : 반드시 요청이 발생하면 응답이 넘어온다.\n",
    "  - 응답 : 200(성공), 400(실패,내탓), 500(실패,서버탓)\n",
    "- 3. BeautifulSoup 라이브러리\n",
    " - string데이터를 html데이터로 변환 (parser를 통해서 parsing)\n",
    " - 핵심 : 컴퓨터는 소통하기위해서 HTML이 필요하다. \n",
    "- 4. 특정 요소를 수집\n",
    " - soup.select(\"선택자 id,class > * > h2\")\n",
    " - 선택자는 작성할 때 반드시 범위르 최대한 좁힌다.\n",
    " - 이유 : 정확한 데이터를 수집할 확률을 올린다.\n",
    " - 내가 수집할 요소가 id, class(구분자)가 없다면 => 부모자식을 활용하자.\n",
    " - 핵심 : 부모가 구분자가 있을 때 까지 검사한다 => 범위를 좁힌다. \n",
    "- 5. crowling은 모든 사이트가 프로세스는 똑같다. \n",
    " - 1) 데이터 요청\n",
    " - 2) 데이터 변환\n",
    " - 3) 데이터 수집\n",
    " - 4) 데이터 가공\n",
    " - 5) 데이터 활용\n",
    "\n",
    "- 6. 월요일에 학습 내용\n",
    " - 링크 수집 (추가) anchor 태그의 href속성 \n",
    " - 상대경로 vs 절대경로: 수집된 링크가 상대경로이다. => 절대경로로 만들어줘야한다.\n",
    " - DF DataFrame으로 변환\n",
    " - 파일로 저장\n",
    "\n",
    " - 멜론차트 406 오류해결 , 선택자 해결 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6c7cd",
   "metadata": {},
   "source": [
    "##  데이터 확장하기\n",
    "- 타이틀과 함께 같이 파일로 저장하면 좋은 데이터 = 링크\n",
    "- 링크는 anchor 태그에 존재한다 =>href (hyperText reference)속성을 수집\n",
    "- 속성 수집하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a태그를 수집\n",
    "## soup.select(\"div.assetText > a\")\n",
    "\n",
    "\n",
    "# a태그를 수잡한 것을 aTag변수에 넣는다\n",
    "## aTag = soup.select(\"div.assetText > a\")\n",
    "\n",
    "## ★★★★★중복되는숫자가 있으므로 다시 잘 가지고와야한다. 20개 VS 24개★★★★★ \n",
    "## ★★★★★사이트 마다 구조가 다르므로 선택자를 잘 찾아야한다. 중복이 안되게 조상까지 올라가야한다.★★★★★\n",
    "aTag = soup.select(\"div.news_box > div.newsPost > div.assetText > a\")\n",
    "aTag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★★★★★속성수집!!★★★★★\n",
    "# 수집한 a태그에서 href 속성을 수집한다. // 속성의에서 href를 꺼내오던지 h1 꺼내오던지 다 가능하다! \n",
    "aTag[0][\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff00dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문을 통해서 경로만 저장하는 리스트 제작\n",
    "# 나는 url_list에 데이터를 넣을건데, aTag에 있는 데이터에서 href만 하나씩 넣을꺼야.\n",
    "url_list = []\n",
    "## for i in range(len(aTag)):\n",
    "##     url_list.append(aTag[i][\"href\"])\n",
    "\n",
    "# 현재의 url_list는 상대경로이다. 따라서 절대경로로 변경해줘야한다. \n",
    "## https://zdnet.co.kr/ + 상대경로\n",
    "# 방법 : 조회한 사이트의 서버주소 (com, net, co.kr등) 뒤에 상대경로를 더해주자.\n",
    "# 속성수집!!(href, src, class, id 등등)\n",
    "for i in range(len(aTag)):\n",
    "    url_list.append(\"https://zdnet.co.kr/\"+aTag[i][\"href\"])\n",
    "\n",
    "# 갯수와 link와 갯수검증을 해야한다. \n",
    "# 핵심 : 데이터의 개수가 중요한 경우에는 반드시 개수를 검증 \n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★★★★★title 갯수와  url 갯수검증을 해야한다.★★★★★ \n",
    "# 핵심 : 데이터의 개수가 중요한 경우에는 반드시 개수를 검증\n",
    "# 선택자를 잘 찾아야한다. 중복이 안되게 조상까지 올라가야한다. \n",
    "# 해결책 : 중복되는 선택자를 찾는다. => 중복이 되지 않을 때 까지 작성한다.\n",
    "# 선택자에서 계층선택자가 매우 중요하다.\n",
    "\n",
    "print(len(title_list), len(url_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d03fb",
   "metadata": {},
   "source": [
    "## 데이터 확장하기 2\n",
    "- 현재는 한페이지에서만 데이터를 수집 가능(20개)\n",
    "- 크롤링은 많은 양의 데이터를 수집할 때 효율적\n",
    "- 다음 페이지를 수집하는 미션\n",
    "- 1) 2페이지를 클릭 (req)로는 불가능하다. => 조작이 안되기 때문에.\n",
    "- 2) url의 변하는 값을 활용한다. https://zdnet.co.kr/news/?lstcode=0000&page=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2aa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 2페이지의 url로 사이트 요청하기 => req.get()\n",
    "res = req.get(\"https://zdnet.co.kr/news/?lstcode=0000&page=2\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b5987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. bs으로 데이터 파싱하기\n",
    "soup2 = bs(res.text, \"lxml\")\n",
    "soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1. 제목, 링크 수집하기 (h3 )\n",
    "header2 = soup2.select(\"div.left_cont> div.news_box > div.newsPost > div.assetText > a > h3\")\n",
    "print(len(header2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2. 제목, 링크 수집하기 (h3와 anchor )\n",
    "aTag2 = soup2.select(\"div.left_cont > div.news_box > div.newsPost > div.assetText > a\")\n",
    "print(len(aTag2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bbb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 비어있는 리스트 제작 후, 컨텐츠와 href만 저장 \n",
    "# a tag 안에 있는 속성(href)을 저장한다 \n",
    "title_list2 = []\n",
    "url_list2 = []\n",
    "website = \"http://zdnet.co.kr\"\n",
    "\n",
    "for i in range(len(aTag2)):\n",
    "    title_list2.append(title[i].text) \n",
    "    url_list2.append(website + aTag2[i][\"href\"])\n",
    "\n",
    "print(url_list2)\n",
    "\n",
    "title_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b92e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11753df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 개수체크 20개씩\n",
    "print(len(header2),len(url_list2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebee005",
   "metadata": {},
   "source": [
    "# 반복하는 코드와 변화하는 부분을 찾아서 합치기\n",
    "- 10 페이지 수집하는 코드를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3ad754ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 19의 제목 개수 : 200, 링크 개수 : 200\n",
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "# page를 i는 1부터 시작할것이니깐  \n",
    "# 초기화가 되어버린다, 결과가 20개밖에 안나온다. \n",
    "# 반복하지 않는 코드를 밖에서 1번만 실행하게 해줘야한다 .(초기화)\n",
    "\n",
    "\n",
    "# 맨나중에 생각하기. title_list = []\n",
    "title_list = []\n",
    "# 맨나중에 생각하기. url_list = []\n",
    "url_list = []\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # 데이터 요청\n",
    "    res = req.get(f\"https://zdnet.co.kr/news/?lstcode=0000&page={i}\")\n",
    "    soup= bs(res.text, \"lxml\")\n",
    "\n",
    "    #데이터 수집\n",
    "    title = soup.select(\"div.left_cont > div.news_box > div.newsPost > div.assetText > a > h3\")\n",
    "    aTag = soup.select(\"div.left_cont > div.news_box > div.newsPost > div.assetText > a\")\n",
    "\n",
    "    #데이터 가공\n",
    "    for i in range(len(title)):\n",
    "        title_list.append(title[i].text) \n",
    "        url_list.append(\"https://zdnet.co.kr/\" + aTag[i][\"href\"])\n",
    "\n",
    "print(f\"페이지 {i}의 제목 개수 : {len(title_list)}, 링크 개수 : {len(url_list)}\")\n",
    "print(type(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fef5a7",
   "metadata": {},
   "source": [
    "## df으로 시각화, 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e462a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517bca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스제목 200개 , url 200개 \n",
    "# data를 가지고있고, calumn이 없다.\n",
    "# 200개를 같이 묶어서 관리하겠다. \n",
    "# type이 동일하다 => list가 좋겠다. \n",
    "# title과 url은 type이 다르다 => dict가 좋겠다.\n",
    "\n",
    "# 표를 제작하기 위해서 컬럼과 데이터구조의 형태로 제작한다.\n",
    "data = { \"뉴스제목\" : title_list, \"뉴스링크\" : url_list} \n",
    "\n",
    "# df령식으로 변환시켜야한다. pandas 이용\n",
    "pd.DataFrame(data)\n",
    "\n",
    "# 변수에 담는다.\n",
    "news =pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6249a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장 (html)\n",
    "# 가져온다 form 내보낸다 to \n",
    "news.to_html(\"./뉴스제목.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장 (CSV, encoding= \"utf-8-sig\" (signature)) \n",
    "# 한글이 포함된 파일을 저장할 대는 encoding을 신경쓰자! \n",
    "news.to_csv(\"./뉴스제목.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700b990",
   "metadata": {},
   "source": [
    "# 클롤링 오전수업\n",
    "## 1. 속성수집(href, src, class, id 등등)\n",
    "- 속성을 가지고 있는 태그를 먼저 수집한다.\n",
    "- 속성을 접근하기 위해서는 [속성명]\n",
    "\n",
    "## 2.반복문 짜는 법\n",
    "- 핵심 : 문법으로 접근하지 말고, 완벽한 하나의 사이클을 만들자.\n",
    "- 반복할 코드와 반복하지 않을 코드를 분리한다.\n",
    "- 반복문으로 묶어주고 마지막에 i의 위치를 찾아준다.\n",
    "## 3. req로 여러페이지를 수집하기 위해서는 url의 변화에 집중하자.\n",
    "- 요청할 대 마다, 변하는 url로 재요청 해결이 가능\n",
    "## 4. 응답코드가 400번대 경우 해결책\n",
    "- 400번대 오류는 요청하는 나의 문재\n",
    "- 1) 잘못된 url을 요청했을 때\n",
    "- 2) 브라우저가 아님을 감지 당할때.\n",
    "- user-agnet코드를 요청할 때 같이 보낸다.\n",
    "\n",
    "## 5. 오후 실습\n",
    "- 가수, 노래 수집 (선택자)\n",
    "- 환율정보사이트 수집(수집불가) = res 200 but no data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
