"""
3-Part ë°ì´í„° ë°±ì—… ë° ë™ê¸°í™” ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ 3-Part ì¼ì¼ ë°˜ì„± ì‹œìŠ¤í…œì˜ ë°ì´í„° ë°±ì—…ê³¼
Notion-ë¡œì»¬ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import json
import os
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import hashlib
import sqlite3
from pathlib import Path

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from src.notion_automation.utils.logger import ThreePartLogger

class ThreePartBackupSystem:
    """3-Part ì‹œìŠ¤í…œ ë°ì´í„° ë°±ì—… í´ë˜ìŠ¤"""
    
    def __init__(self, logger: Optional[ThreePartLogger] = None):
        """
        ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            logger: ë¡œê¹… ì‹œìŠ¤í…œ
        """
        self.logger = logger or ThreePartLogger(name="backup_system")
        
        # ë°±ì—… ë””ë ‰í„°ë¦¬ ì„¤ì •
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
        self.backup_root = os.path.join(project_root, "data", "backups")
        self.daily_backup_dir = os.path.join(self.backup_root, "daily")
        self.weekly_backup_dir = os.path.join(self.backup_root, "weekly")
        self.monthly_backup_dir = os.path.join(self.backup_root, "monthly")
        
        # ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        self.local_db_path = os.path.join(project_root, "data", "3part_local.db")
        
        # ë””ë ‰í„°ë¦¬ ìƒì„±
        self._create_backup_directories()
        
        # ë¡œì»¬ DB ì´ˆê¸°í™”
        self._initialize_local_database()
        
        self.logger.info("3-Part ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_backup_directories(self):
        """ë°±ì—… ë””ë ‰í„°ë¦¬ ìƒì„±"""
        for directory in [self.daily_backup_dir, self.weekly_backup_dir, self.monthly_backup_dir]:
            os.makedirs(directory, exist_ok=True)
    
    def _initialize_local_database(self):
        """ë¡œì»¬ SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.local_db_path) as conn:
                cursor = conn.cursor()
                
                # 3-Part ë°ì´í„° í…Œì´ë¸” ìƒì„±
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS three_part_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        date TEXT NOT NULL,
                        time_part TEXT NOT NULL,
                        focus_level INTEGER,
                        understanding_level INTEGER,
                        fatigue_level INTEGER,
                        satisfaction_level INTEGER,
                        difficulty_level INTEGER,
                        study_amount INTEGER,
                        notes TEXT,
                        github_commits INTEGER DEFAULT 0,
                        github_prs INTEGER DEFAULT 0,
                        github_issues INTEGER DEFAULT 0,
                        data_hash TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(date, time_part)
                    )
                ''')
                
                # ë°±ì—… íˆìŠ¤í† ë¦¬ í…Œì´ë¸” ìƒì„±
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS backup_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        backup_type TEXT NOT NULL,
                        backup_date TEXT NOT NULL,
                        file_path TEXT NOT NULL,
                        file_size INTEGER,
                        record_count INTEGER,
                        backup_hash TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # ë™ê¸°í™” ë¡œê·¸ í…Œì´ë¸” ìƒì„±
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS sync_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        sync_date TEXT NOT NULL,
                        source TEXT NOT NULL,
                        target TEXT NOT NULL,
                        action TEXT NOT NULL,
                        record_count INTEGER,
                        status TEXT,
                        error_message TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                conn.commit()
                self.logger.info("ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def calculate_data_hash(self, data: Dict[str, Any]) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        # ë°ì´í„°ë¥¼ ì •ë ¬ëœ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        sorted_data = json.dumps(data, sort_keys=True, ensure_ascii=False)
        
        # SHA-256 í•´ì‹œ ê³„ì‚°
        hash_object = hashlib.sha256(sorted_data.encode('utf-8'))
        return hash_object.hexdigest()
    
    def save_local_data(self, date: str, time_part: str, data: Dict[str, Any]) -> bool:
        """
        ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì— 3-Part ë°ì´í„° ì €ì¥
        
        Args:
            date: ë‚ ì§œ (YYYY-MM-DD)
            time_part: ì‹œê°„ëŒ€ (morning/afternoon/evening)
            data: ì €ì¥í•  ë°ì´í„°
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            data_hash = self.calculate_data_hash(data)
            
            with sqlite3.connect(self.local_db_path) as conn:
                cursor = conn.cursor()
                
                # UPSERT ì‘ì—… (INSERT OR REPLACE)
                cursor.execute('''
                    INSERT OR REPLACE INTO three_part_data 
                    (date, time_part, focus_level, understanding_level, fatigue_level,
                     satisfaction_level, difficulty_level, study_amount, notes,
                     github_commits, github_prs, github_issues, data_hash, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                ''', (
                    date, time_part,
                    data.get('focus_level'),
                    data.get('understanding_level'),
                    data.get('fatigue_level'),
                    data.get('satisfaction_level'),
                    data.get('difficulty_level'),
                    data.get('study_amount'),
                    data.get('notes'),
                    data.get('github_commits', 0),
                    data.get('github_prs', 0),
                    data.get('github_issues', 0),
                    data_hash
                ))
                
                conn.commit()
                self.logger.info(f"ë¡œì»¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {date} {time_part}")
                return True
                
        except Exception as e:
            self.logger.error(f"ë¡œì»¬ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def load_local_data(self, date_from: str, date_to: str) -> List[Dict[str, Any]]:
        """
        ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ 3-Part ë°ì´í„° ë¡œë“œ
        
        Args:
            date_from: ì‹œì‘ ë‚ ì§œ
            date_to: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            ë¡œë“œëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            with sqlite3.connect(self.local_db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT date, time_part, focus_level, understanding_level, fatigue_level,
                           satisfaction_level, difficulty_level, study_amount, notes,
                           github_commits, github_prs, github_issues, data_hash,
                           created_at, updated_at
                    FROM three_part_data
                    WHERE date BETWEEN ? AND ?
                    ORDER BY date DESC, 
                             CASE time_part 
                                 WHEN 'morning' THEN 1 
                                 WHEN 'afternoon' THEN 2 
                                 WHEN 'evening' THEN 3 
                             END
                ''', (date_from, date_to))
                
                rows = cursor.fetchall()
                
                data_list = []
                for row in rows:
                    data_dict = {
                        'date': row[0],
                        'time_part': row[1],
                        'focus_level': row[2],
                        'understanding_level': row[3],
                        'fatigue_level': row[4],
                        'satisfaction_level': row[5],
                        'difficulty_level': row[6],
                        'study_amount': row[7],
                        'notes': row[8],
                        'github_commits': row[9],
                        'github_prs': row[10],
                        'github_issues': row[11],
                        'data_hash': row[12],
                        'created_at': row[13],
                        'updated_at': row[14]
                    }
                    data_list.append(data_dict)
                
                self.logger.info(f"ë¡œì»¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data_list)}ê°œ ë ˆì½”ë“œ")
                return data_list
                
        except Exception as e:
            self.logger.error(f"ë¡œì»¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def create_daily_backup(self, date: Optional[str] = None) -> str:
        """
        ì¼ì¼ ë°±ì—… ìƒì„±
        
        Args:
            date: ë°±ì—…í•  ë‚ ì§œ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
            
        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        self.logger.info(f"ì¼ì¼ ë°±ì—… ì‹œì‘: {date}")
        
        try:
            # í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„° ë¡œë“œ
            data = self.load_local_data(date, date)
            
            # ë°±ì—… íŒŒì¼ëª… ìƒì„±
            backup_filename = f"3part_daily_backup_{date}.json"
            backup_filepath = os.path.join(self.daily_backup_dir, backup_filename)
            
            # ë°±ì—… ë°ì´í„° êµ¬ì¡° ìƒì„±
            backup_data = {
                "backup_info": {
                    "backup_type": "daily",
                    "backup_date": date,
                    "created_at": datetime.now().isoformat(),
                    "record_count": len(data)
                },
                "data": data
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(backup_filepath, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            # ë°±ì—… íˆìŠ¤í† ë¦¬ ê¸°ë¡
            file_size = os.path.getsize(backup_filepath)
            backup_hash = self.calculate_data_hash(backup_data)
            
            self._record_backup_history("daily", date, backup_filepath, file_size, len(data), backup_hash)
            
            self.logger.info(f"ì¼ì¼ ë°±ì—… ì™„ë£Œ: {backup_filepath} ({file_size} bytes)")
            return backup_filepath
            
        except Exception as e:
            self.logger.error(f"ì¼ì¼ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
            return ""
    
    def create_weekly_backup(self, week_start_date: Optional[str] = None) -> str:
        """
        ì£¼ê°„ ë°±ì—… ìƒì„±
        
        Args:
            week_start_date: ì£¼ê°„ ì‹œì‘ ë‚ ì§œ (ê¸°ë³¸ê°’: ì´ë²ˆ ì£¼ ì›”ìš”ì¼)
            
        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        if week_start_date is None:
            # ì´ë²ˆ ì£¼ ì›”ìš”ì¼ ê³„ì‚°
            today = datetime.now()
            days_since_monday = today.weekday()
            monday = today - timedelta(days=days_since_monday)
            week_start_date = monday.strftime("%Y-%m-%d")
        
        # ì£¼ê°„ ì¢…ë£Œ ë‚ ì§œ ê³„ì‚°
        start_date = datetime.strptime(week_start_date, "%Y-%m-%d")
        end_date = start_date + timedelta(days=6)
        week_end_date = end_date.strftime("%Y-%m-%d")
        
        self.logger.info(f"ì£¼ê°„ ë°±ì—… ì‹œì‘: {week_start_date} ~ {week_end_date}")
        
        try:
            # ì£¼ê°„ ë°ì´í„° ë¡œë“œ
            data = self.load_local_data(week_start_date, week_end_date)
            
            # ë°±ì—… íŒŒì¼ëª… ìƒì„±
            backup_filename = f"3part_weekly_backup_{week_start_date}_{week_end_date}.json"
            backup_filepath = os.path.join(self.weekly_backup_dir, backup_filename)
            
            # ë°±ì—… ë°ì´í„° êµ¬ì¡° ìƒì„±
            backup_data = {
                "backup_info": {
                    "backup_type": "weekly",
                    "week_start": week_start_date,
                    "week_end": week_end_date,
                    "created_at": datetime.now().isoformat(),
                    "record_count": len(data)
                },
                "data": data
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(backup_filepath, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            # ë°±ì—… íˆìŠ¤í† ë¦¬ ê¸°ë¡
            file_size = os.path.getsize(backup_filepath)
            backup_hash = self.calculate_data_hash(backup_data)
            
            self._record_backup_history("weekly", f"{week_start_date}_{week_end_date}", 
                                      backup_filepath, file_size, len(data), backup_hash)
            
            self.logger.info(f"ì£¼ê°„ ë°±ì—… ì™„ë£Œ: {backup_filepath} ({file_size} bytes)")
            return backup_filepath
            
        except Exception as e:
            self.logger.error(f"ì£¼ê°„ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
            return ""
    
    def _record_backup_history(self, backup_type: str, backup_date: str, 
                              file_path: str, file_size: int, record_count: int, backup_hash: str):
        """ë°±ì—… íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        try:
            with sqlite3.connect(self.local_db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO backup_history 
                    (backup_type, backup_date, file_path, file_size, record_count, backup_hash)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (backup_type, backup_date, file_path, file_size, record_count, backup_hash))
                
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"ë°±ì—… íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹¤íŒ¨: {str(e)}")
    
    def verify_backup_integrity(self, backup_filepath: str) -> Dict[str, Any]:
        """
        ë°±ì—… íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
        
        Args:
            backup_filepath: ê²€ì¦í•  ë°±ì—… íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        self.logger.info(f"ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘: {backup_filepath}")
        
        result = {
            "file_exists": False,
            "file_readable": False,
            "json_valid": False,
            "data_integrity": False,
            "hash_match": False,
            "record_count_match": False,
            "errors": []
        }
        
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(backup_filepath):
                result["errors"].append("ë°±ì—… íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return result
            
            result["file_exists"] = True
            
            # íŒŒì¼ ì½ê¸° ê°€ëŠ¥ í™•ì¸
            try:
                with open(backup_filepath, 'r', encoding='utf-8') as f:
                    backup_data = json.load(f)
                result["file_readable"] = True
                result["json_valid"] = True
            except Exception as e:
                result["errors"].append(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
                return result
            
            # ë°ì´í„° êµ¬ì¡° ê²€ì¦
            if "backup_info" in backup_data and "data" in backup_data:
                result["data_integrity"] = True
                
                # ë ˆì½”ë“œ ê°œìˆ˜ í™•ì¸
                expected_count = backup_data["backup_info"].get("record_count", 0)
                actual_count = len(backup_data["data"])
                
                if expected_count == actual_count:
                    result["record_count_match"] = True
                else:
                    result["errors"].append(
                        f"ë ˆì½”ë“œ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_count}, ì‹¤ì œ {actual_count}"
                    )
                
                # í•´ì‹œ ê²€ì¦ (ê°€ëŠ¥í•œ ê²½ìš°)
                current_hash = self.calculate_data_hash(backup_data)
                result["current_hash"] = current_hash
                result["hash_match"] = True  # ìƒˆë¡œ ê³„ì‚°ëœ í•´ì‹œì´ë¯€ë¡œ í•­ìƒ ì¼ì¹˜
            else:
                result["errors"].append("ë°±ì—… ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            result["success"] = (
                result["file_exists"] and 
                result["file_readable"] and 
                result["json_valid"] and 
                result["data_integrity"] and 
                result["record_count_match"]
            )
            
            self.logger.info(f"ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ: {result['success']}")
            
        except Exception as e:
            result["errors"].append(f"ê²€ì¦ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            self.logger.error(f"ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        
        return result
    
    def sync_with_notion_data(self, notion_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Notion ë°ì´í„°ì™€ ë¡œì»¬ ë°ì´í„° ë™ê¸°í™”
        
        Args:
            notion_data: Notionì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°
            
        Returns:
            ë™ê¸°í™” ê²°ê³¼
        """
        self.logger.info(f"Notion ë°ì´í„° ë™ê¸°í™” ì‹œì‘: {len(notion_data)}ê°œ ë ˆì½”ë“œ")
        
        sync_result = {
            "notion_records": len(notion_data),
            "local_records": 0,
            "new_records": 0,
            "updated_records": 0,
            "conflicts": 0,
            "errors": []
        }
        
        try:
            # ê¸°ì¡´ ë¡œì»¬ ë°ì´í„° ë¡œë“œ
            date_range_start = min(item.get("date", "9999-12-31") for item in notion_data) if notion_data else "2025-01-01"
            date_range_end = max(item.get("date", "1900-01-01") for item in notion_data) if notion_data else "2025-12-31"
            
            local_data = self.load_local_data(date_range_start, date_range_end)
            sync_result["local_records"] = len(local_data)
            
            # ë¡œì»¬ ë°ì´í„°ë¥¼ í‚¤ë¡œ ì¸ë±ì‹±
            local_index = {}
            for item in local_data:
                key = f"{item['date']}_{item['time_part']}"
                local_index[key] = item
            
            # Notion ë°ì´í„° ì²˜ë¦¬
            for notion_item in notion_data:
                date = notion_item.get("date")
                time_part = notion_item.get("time_part")
                
                if not date or not time_part:
                    sync_result["errors"].append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {notion_item}")
                    continue
                
                key = f"{date}_{time_part}"
                
                if key in local_index:
                    # ê¸°ì¡´ ë ˆì½”ë“œ - ì—…ë°ì´íŠ¸ í™•ì¸
                    local_item = local_index[key]
                    notion_hash = self.calculate_data_hash(notion_item)
                    local_hash = local_item.get("data_hash", "")
                    
                    if notion_hash != local_hash:
                        # ë°ì´í„°ê°€ ë‹¤ë¦„ - ì—…ë°ì´íŠ¸
                        if self.save_local_data(date, time_part, notion_item):
                            sync_result["updated_records"] += 1
                        else:
                            sync_result["errors"].append(f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {key}")
                else:
                    # ìƒˆë¡œìš´ ë ˆì½”ë“œ
                    if self.save_local_data(date, time_part, notion_item):
                        sync_result["new_records"] += 1
                    else:
                        sync_result["errors"].append(f"ì‹ ê·œ ì €ì¥ ì‹¤íŒ¨: {key}")
            
            # ë™ê¸°í™” ë¡œê·¸ ê¸°ë¡
            self._record_sync_log(
                sync_date=datetime.now().strftime("%Y-%m-%d"),
                source="notion",
                target="local",
                action="sync",
                record_count=len(notion_data),
                status="success" if not sync_result["errors"] else "partial_success"
            )
            
            self.logger.info(f"Notion ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {sync_result}")
            
        except Exception as e:
            sync_result["errors"].append(f"ë™ê¸°í™” ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            self.logger.error(f"Notion ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        return sync_result
    
    def _record_sync_log(self, sync_date: str, source: str, target: str, 
                        action: str, record_count: int, status: str, error_message: Optional[str] = None):
        """ë™ê¸°í™” ë¡œê·¸ ê¸°ë¡"""
        try:
            with sqlite3.connect(self.local_db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO sync_log 
                    (sync_date, source, target, action, record_count, status, error_message)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (sync_date, source, target, action, record_count, status, error_message))
                
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"ë™ê¸°í™” ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {str(e)}")
    
    def get_backup_statistics(self) -> Dict[str, Any]:
        """ë°±ì—… í†µê³„ ë°˜í™˜"""
        try:
            with sqlite3.connect(self.local_db_path) as conn:
                cursor = conn.cursor()
                
                # ë°±ì—… íˆìŠ¤í† ë¦¬ í†µê³„
                cursor.execute('''
                    SELECT backup_type, COUNT(*) as count, 
                           SUM(file_size) as total_size,
                           AVG(record_count) as avg_records
                    FROM backup_history
                    GROUP BY backup_type
                ''')
                
                backup_stats = {}
                for row in cursor.fetchall():
                    backup_stats[row[0]] = {
                        "count": row[1],
                        "total_size_bytes": row[2] or 0,
                        "average_records": round(row[3] or 0, 2)
                    }
                
                # ìµœê·¼ ë°±ì—… ì •ë³´
                cursor.execute('''
                    SELECT backup_type, backup_date, file_path, created_at
                    FROM backup_history
                    ORDER BY created_at DESC
                    LIMIT 5
                ''')
                
                recent_backups = []
                for row in cursor.fetchall():
                    recent_backups.append({
                        "type": row[0],
                        "date": row[1],
                        "file_path": row[2],
                        "created_at": row[3]
                    })
                
                return {
                    "backup_statistics": backup_stats,
                    "recent_backups": recent_backups,
                    "total_backups": sum(stats["count"] for stats in backup_stats.values())
                }
                
        except Exception as e:
            self.logger.error(f"ë°±ì—… í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {}


def main():
    """3-Part ë°±ì—… ë° ë™ê¸°í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ’¾ 3-Part ë°ì´í„° ë°±ì—… ë° ë™ê¸°í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 55)
    
    # ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    backup_system = ThreePartBackupSystem()
    
    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì €ì¥
    print("\nğŸ“ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ìµœê·¼ 7ì¼ê°„ì˜ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    sample_data = []
    for i in range(7):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
        
        for time_part in ["morning", "afternoon", "evening"]:
            data = {
                "date": date,
                "time_part": time_part,
                "focus_level": 5 + (i % 5),
                "understanding_level": 6 + (i % 4),
                "fatigue_level": 3 + (i % 3),
                "satisfaction_level": 7 + (i % 3),
                "difficulty_level": 4 + (i % 4),
                "study_amount": 2 + (i % 3),
                "notes": f"í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ {date} {time_part}",
                "github_commits": i % 5,
                "github_prs": i % 2,
                "github_issues": i % 3
            }
            
            # ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            backup_system.save_local_data(date, time_part, data)
            sample_data.append(data)
    
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(sample_data)}ê°œ ë ˆì½”ë“œ")
    
    # 2. ì¼ì¼ ë°±ì—… í…ŒìŠ¤íŠ¸
    print("\nğŸ“… ì¼ì¼ ë°±ì—… í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    today = datetime.now().strftime("%Y-%m-%d")
    daily_backup_path = backup_system.create_daily_backup(today)
    
    if daily_backup_path:
        print(f"âœ… ì¼ì¼ ë°±ì—… ìƒì„± ì™„ë£Œ: {daily_backup_path}")
        
        # ë°±ì—… ë¬´ê²°ì„± ê²€ì¦
        integrity_result = backup_system.verify_backup_integrity(daily_backup_path)
        if integrity_result["success"]:
            print("âœ… ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
        else:
            print(f"âŒ ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {integrity_result['errors']}")
    else:
        print("âŒ ì¼ì¼ ë°±ì—… ìƒì„± ì‹¤íŒ¨")
    
    # 3. ì£¼ê°„ ë°±ì—… í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š ì£¼ê°„ ë°±ì—… í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    weekly_backup_path = backup_system.create_weekly_backup()
    
    if weekly_backup_path:
        print(f"âœ… ì£¼ê°„ ë°±ì—… ìƒì„± ì™„ë£Œ: {weekly_backup_path}")
        
        # ë°±ì—… ë¬´ê²°ì„± ê²€ì¦
        integrity_result = backup_system.verify_backup_integrity(weekly_backup_path)
        if integrity_result["success"]:
            print("âœ… ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
        else:
            print(f"âŒ ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {integrity_result['errors']}")
    else:
        print("âŒ ì£¼ê°„ ë°±ì—… ìƒì„± ì‹¤íŒ¨")
    
    # 4. Notion ë™ê¸°í™” ì‹œë®¬ë ˆì´ì…˜
    print("\nğŸ”„ Notion ë™ê¸°í™” ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
    
    # Notionì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì²˜ëŸ¼ ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„° (ì¼ë¶€ ë³€ê²½ì‚¬í•­ í¬í•¨)
    notion_data = sample_data.copy()
    
    # ì¼ë¶€ ë°ì´í„° ìˆ˜ì • (ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
    if notion_data:
        notion_data[0]["focus_level"] = 9  # ë³€ê²½ì‚¬í•­
        notion_data[1]["notes"] = "Notionì—ì„œ ìˆ˜ì •ëœ ë…¸íŠ¸"  # ë³€ê²½ì‚¬í•­
        
        # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
        new_data = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "time_part": "evening",
            "focus_level": 8,
            "understanding_level": 8,
            "fatigue_level": 2,
            "satisfaction_level": 9,
            "difficulty_level": 5,
            "study_amount": 4,
            "notes": "ìƒˆë¡œìš´ Notion ë°ì´í„°",
            "github_commits": 3,
            "github_prs": 1,
            "github_issues": 0
        }
        notion_data.append(new_data)
    
    sync_result = backup_system.sync_with_notion_data(notion_data)
    
    print(f"ğŸ”„ ë™ê¸°í™” ê²°ê³¼:")
    print(f"  - Notion ë ˆì½”ë“œ: {sync_result['notion_records']}ê°œ")
    print(f"  - ë¡œì»¬ ë ˆì½”ë“œ: {sync_result['local_records']}ê°œ")
    print(f"  - ì‹ ê·œ ë ˆì½”ë“œ: {sync_result['new_records']}ê°œ")
    print(f"  - ì—…ë°ì´íŠ¸ ë ˆì½”ë“œ: {sync_result['updated_records']}ê°œ")
    
    if sync_result["errors"]:
        print(f"  - ì—ëŸ¬: {len(sync_result['errors'])}ê°œ")
        for error in sync_result["errors"][:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
            print(f"    â€¢ {error}")
    else:
        print("  âœ… ì—ëŸ¬ ì—†ìŒ")
    
    # 5. ë°±ì—… í†µê³„ ì¡°íšŒ
    print("\nğŸ“ˆ ë°±ì—… ì‹œìŠ¤í…œ í†µê³„:")
    
    stats = backup_system.get_backup_statistics()
    
    if stats:
        print(f"  ğŸ“Š ì´ ë°±ì—… ìˆ˜: {stats.get('total_backups', 0)}ê°œ")
        
        backup_stats = stats.get("backup_statistics", {})
        for backup_type, stat in backup_stats.items():
            size_mb = stat["total_size_bytes"] / 1024 / 1024
            print(f"  - {backup_type}: {stat['count']}ê°œ, {size_mb:.2f}MB")
        
        recent_backups = stats.get("recent_backups", [])
        if recent_backups:
            print(f"\n  ğŸ•’ ìµœê·¼ ë°±ì—… (ìµœëŒ€ 3ê°œ):")
            for backup in recent_backups[:3]:
                print(f"    â€¢ {backup['type']}: {backup['date']} ({backup['created_at']})")
    
    print("\n" + "=" * 55)
    print("ğŸ‰ Task 6.1.3 (3-Part ë°ì´í„° ë°±ì—… ë° ë™ê¸°í™”) êµ¬í˜„ ì™„ë£Œ!")
    print("âœ… ëª©í‘œ ë‹¬ì„±: ë°ì´í„° ì†ì‹¤ ë°©ì§€ ì‹œìŠ¤í…œ ì™„ì„±")


if __name__ == "__main__":
    main()
