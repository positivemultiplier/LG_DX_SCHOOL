"""
3-Part API í˜¸ì¶œ ìµœì í™” ë° ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ì˜¤ì „/ì˜¤í›„/ì €ë… 3ê°œ ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê³  ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from src.notion_automation.utils.logger import ThreePartLogger

class ThreePartBatchProcessor:
    """3-Part ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ ë° ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, logger: Optional[ThreePartLogger] = None):
        """
        ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            logger: ë¡œê¹… ì‹œìŠ¤í…œ (ì„ íƒì‚¬í•­)
        """
        self.logger = logger or ThreePartLogger(name="batch_processor")
        self.time_parts = ["morning", "afternoon", "evening"]
        self.time_schedules = {
            "morning": {"start": "09:00", "end": "12:00"},
            "afternoon": {"start": "13:00", "end": "17:00"},
            "evening": {"start": "19:00", "end": "22:00"}
        }
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.cache = {}
        self.cache_expiry = {}
        self.cache_duration = 300  # 5ë¶„ ìºì‹œ
        
        self.logger.info("3-Part ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def clear_expired_cache(self):
        """ë§Œë£Œëœ ìºì‹œ í•­ëª© ì •ë¦¬"""
        current_time = time.time()
        expired_keys = [
            key for key, expiry in self.cache_expiry.items()
            if current_time > expiry
        ]
        
        for key in expired_keys:
            del self.cache[key]
            del self.cache_expiry[key]
        
        if expired_keys:
            self.logger.info(f"ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ í•­ëª© ì •ë¦¬")
    
    def get_from_cache(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        self.clear_expired_cache()
        return self.cache.get(key)
    
    def set_cache(self, key: str, value: Any):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        self.cache[key] = value
        self.cache_expiry[key] = time.time() + self.cache_duration
    
    def generate_sample_3part_data(self, days: int = 7) -> Dict[str, List[Dict]]:
        """
        3-Part ì‹œìŠ¤í…œìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        
        Args:
            days: ìƒì„±í•  ì¼ìˆ˜
            
        Returns:
            ì‹œê°„ëŒ€ë³„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        import random
        
        sample_data = {
            "morning": [],
            "afternoon": [],
            "evening": []
        }
        
        for i in range(days):
            date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            
            for time_part in self.time_parts:
                # ì‹œê°„ëŒ€ë³„ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë°ì´í„° ìƒì„±
                base_scores = {
                    "morning": {"focus": 8, "energy": 9, "fatigue": 2},
                    "afternoon": {"focus": 7, "energy": 7, "fatigue": 4},
                    "evening": {"focus": 6, "energy": 5, "fatigue": 6}
                }
                
                base = base_scores[time_part]
                variation = random.uniform(-2, 2)
                
                entry = {
                    "date": date,
                    "time_part": time_part,
                    "focus_level": max(1, min(10, base["focus"] + variation)),
                    "understanding_level": max(1, min(10, base["focus"] + random.uniform(-1, 1))),
                    "fatigue_level": max(1, min(10, base["fatigue"] + random.uniform(-1, 1))),
                    "satisfaction_level": max(1, min(10, base["focus"] + random.uniform(-1.5, 1.5))),
                    "difficulty_level": random.randint(3, 8),
                    "study_amount": random.randint(1, 5),
                    "github_commits": random.randint(0, 8),
                    "github_prs": random.randint(0, 2),
                    "github_issues": random.randint(0, 3)
                }
                
                sample_data[time_part].append(entry)
        
        return sample_data
    
    def load_3part_data_batch(self, date_range: int = 7) -> Dict[str, List[Dict]]:
        """
        3ê°œ ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë¡œë“œ
        
        Args:
            date_range: ë¡œë“œí•  ë‚ ì§œ ë²”ìœ„ (ì¼)
            
        Returns:
            ì‹œê°„ëŒ€ë³„ ë°ì´í„°
        """
        cache_key = f"3part_data_batch_{date_range}"
        cached_data = self.get_from_cache(cache_key)
        
        if cached_data:
            self.logger.info(f"ìºì‹œì—ì„œ 3-Part ë°ì´í„° ë¡œë“œ (ìºì‹œ íˆíŠ¸)")
            return cached_data
        
        self.logger.info(f"3-Part ë°ì´í„° ë°°ì¹˜ ë¡œë“œ ì‹œì‘ (ìµœê·¼ {date_range}ì¼)")
        start_time = time.time()
        
        try:
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Notion MCPë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
            # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            data = self.generate_sample_3part_data(date_range)
            
            # ìºì‹œì— ì €ì¥
            self.set_cache(cache_key, data)
            
            load_time = time.time() - start_time
            total_entries = sum(len(entries) for entries in data.values())
            
            self.logger.info(f"3-Part ë°ì´í„° ë°°ì¹˜ ë¡œë“œ ì™„ë£Œ: {total_entries}ê°œ ì—”íŠ¸ë¦¬, {load_time:.2f}ì´ˆ")
            
            return data
            
        except Exception as e:
            self.logger.error(f"3-Part ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return {"morning": [], "afternoon": [], "evening": []}
    
    def process_github_data_batch(self, time_part_data: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """
        GitHub ë°ì´í„°ë¥¼ ì‹œê°„ëŒ€ë³„ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        
        Args:
            time_part_data: ì‹œê°„ëŒ€ë³„ ë°ì´í„°
            
        Returns:
            ì²˜ë¦¬ëœ GitHub í†µê³„
        """
        self.logger.info("GitHub ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        start_time = time.time()
        
        github_stats = {}
        
        for time_part, entries in time_part_data.items():
            if not entries:
                continue
                
            # ì‹œê°„ëŒ€ë³„ GitHub í™œë™ í†µê³„ ê³„ì‚°
            total_commits = sum(entry.get("github_commits", 0) for entry in entries)
            total_prs = sum(entry.get("github_prs", 0) for entry in entries)
            total_issues = sum(entry.get("github_issues", 0) for entry in entries)
            
            avg_commits = total_commits / len(entries) if entries else 0
            avg_prs = total_prs / len(entries) if entries else 0
            avg_issues = total_issues / len(entries) if entries else 0
            
            # ìƒì‚°ì„± ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            productivity_score = (
                avg_commits * 1.0 +
                avg_prs * 3.0 +
                avg_issues * 2.0
            )
            
            github_stats[time_part] = {
                "total_commits": total_commits,
                "total_prs": total_prs,
                "total_issues": total_issues,
                "avg_commits": round(avg_commits, 2),
                "avg_prs": round(avg_prs, 2),
                "avg_issues": round(avg_issues, 2),
                "productivity_score": round(productivity_score, 2),
                "activity_days": len(entries)
            }
        
        process_time = time.time() - start_time
        self.logger.info(f"GitHub ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {process_time:.2f}ì´ˆ")
        
        return github_stats
    
    def analyze_3part_performance_batch(self, time_part_data: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """
        3-Part ì„±ê³¼ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë¶„ì„
        
        Args:
            time_part_data: ì‹œê°„ëŒ€ë³„ ë°ì´í„°
            
        Returns:
            ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("3-Part ì„±ê³¼ ë°°ì¹˜ ë¶„ì„ ì‹œì‘")
        start_time = time.time()
        
        performance_stats = {}
        
        for time_part, entries in time_part_data.items():
            if not entries:
                continue
            
            # ê¸°ë³¸ í†µê³„ ê³„ì‚°
            focus_scores = [entry.get("focus_level", 0) for entry in entries]
            understanding_scores = [entry.get("understanding_level", 0) for entry in entries]
            fatigue_scores = [entry.get("fatigue_level", 0) for entry in entries]
            satisfaction_scores = [entry.get("satisfaction_level", 0) for entry in entries]
            
            # íš¨ìœ¨ì„± ì§€ìˆ˜ ê³„ì‚° (ì§‘ì¤‘ë„ / í”¼ë¡œë„ ë¹„ìœ¨)
            efficiency_scores = []
            for entry in entries:
                focus = entry.get("focus_level", 1)
                fatigue = max(entry.get("fatigue_level", 1), 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                efficiency = focus / fatigue
                efficiency_scores.append(efficiency)
            
            performance_stats[time_part] = {
                "avg_focus": round(sum(focus_scores) / len(focus_scores), 2),
                "avg_understanding": round(sum(understanding_scores) / len(understanding_scores), 2),
                "avg_fatigue": round(sum(fatigue_scores) / len(fatigue_scores), 2),
                "avg_satisfaction": round(sum(satisfaction_scores) / len(satisfaction_scores), 2),
                "avg_efficiency": round(sum(efficiency_scores) / len(efficiency_scores), 2),
                "max_focus": max(focus_scores),
                "min_fatigue": min(fatigue_scores),
                "total_sessions": len(entries)
            }
        
        process_time = time.time() - start_time
        self.logger.info(f"3-Part ì„±ê³¼ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {process_time:.2f}ì´ˆ")
        
        return performance_stats
    
    def parallel_process_3part_data(self, date_range: int = 7) -> Dict[str, Any]:
        """
        3-Part ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
        
        Args:
            date_range: ì²˜ë¦¬í•  ë‚ ì§œ ë²”ìœ„
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("3-Part ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
        total_start_time = time.time()
        
        # 1. ë°ì´í„° ë¡œë“œ
        time_part_data = self.load_3part_data_batch(date_range)
        
        if not any(time_part_data.values()):
            self.logger.error("ì²˜ë¦¬í•  3-Part ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        # 2. ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        results = {}
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            # ê° ë¶„ì„ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            future_to_task = {
                executor.submit(self.process_github_data_batch, time_part_data): "github_analysis",
                executor.submit(self.analyze_3part_performance_batch, time_part_data): "performance_analysis",
                executor.submit(self.calculate_optimal_timeparts, time_part_data): "optimal_analysis"
            }
            
            for future in as_completed(future_to_task):
                task_name = future_to_task[future]
                try:
                    result = future.result()
                    results[task_name] = result
                    self.logger.info(f"{task_name} ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"{task_name} ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    results[task_name] = {}
        
        # 3. ê²°ê³¼ í†µí•©
        total_time = time.time() - total_start_time
        
        final_result = {
            "processing_info": {
                "total_time_seconds": round(total_time, 2),
                "processed_date_range": date_range,
                "processing_method": "parallel_batch",
                "cache_enabled": True,
                "timestamp": datetime.now().isoformat()
            },
            "github_stats": results.get("github_analysis", {}),
            "performance_stats": results.get("performance_analysis", {}),
            "optimal_timeparts": results.get("optimal_analysis", {}),
            "summary": self.generate_3part_summary(results)
        }
        
        self.logger.info(f"3-Part ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        return final_result
    
    def calculate_optimal_timeparts(self, time_part_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """
        ìµœì  ì‹œê°„ëŒ€ ê³„ì‚°
        
        Args:
            time_part_data: ì‹œê°„ëŒ€ë³„ ë°ì´í„°
            
        Returns:
            ìµœì  ì‹œê°„ëŒ€ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("ìµœì  ì‹œê°„ëŒ€ ê³„ì‚° ì‹œì‘")
        
        timepart_scores = {}
        
        for time_part, entries in time_part_data.items():
            if not entries:
                continue
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            total_score = 0
            for entry in entries:
                focus = entry.get("focus_level", 0)
                understanding = entry.get("understanding_level", 0)
                fatigue = entry.get("fatigue_level", 10)
                satisfaction = entry.get("satisfaction_level", 0)
                
                # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚° (í”¼ë¡œë„ëŠ” ì—­ì‚°)
                score = (
                    focus * 0.3 +
                    understanding * 0.3 +
                    (10 - fatigue) * 0.2 +
                    satisfaction * 0.2
                )
                total_score += score
            
            avg_score = total_score / len(entries) if entries else 0
            timepart_scores[time_part] = round(avg_score, 2)
        
        # ìµœì /ìµœì € ì‹œê°„ëŒ€ ì‹ë³„
        if timepart_scores:
            best_timepart = max(timepart_scores.keys(), key=lambda k: timepart_scores[k])
            worst_timepart = min(timepart_scores.keys(), key=lambda k: timepart_scores[k])
        else:
            best_timepart = worst_timepart = None
        
        return {
            "timepart_scores": timepart_scores,
            "best_timepart": best_timepart,
            "worst_timepart": worst_timepart,
            "score_difference": timepart_scores.get(best_timepart, 0) - timepart_scores.get(worst_timepart, 0) if best_timepart and worst_timepart else 0
        }
    
    def generate_3part_summary(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        3-Part ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
        
        Args:
            analysis_results: ë¶„ì„ ê²°ê³¼ë“¤
            
        Returns:
            ìš”ì•½ ì •ë³´
        """
        summary = {
            "total_analyzed_timeparts": 3,
            "data_completeness": "high",
            "key_insights": []
        }
        
        # GitHub ë¶„ì„ ìš”ì•½
        github_stats = analysis_results.get("github_analysis", {})
        if github_stats:
            most_productive_time = max(github_stats.keys(), 
                                     key=lambda k: github_stats[k].get("productivity_score", 0))
            summary["most_productive_github_time"] = most_productive_time
            summary["key_insights"].append(f"GitHub í™œë™ì´ ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€: {most_productive_time}")
        
        # ì„±ê³¼ ë¶„ì„ ìš”ì•½
        performance_stats = analysis_results.get("performance_analysis", {})
        if performance_stats:
            most_efficient_time = max(performance_stats.keys(),
                                    key=lambda k: performance_stats[k].get("avg_efficiency", 0))
            summary["most_efficient_time"] = most_efficient_time
            summary["key_insights"].append(f"í•™ìŠµ íš¨ìœ¨ì´ ê°€ì¥ ë†’ì€ ì‹œê°„ëŒ€: {most_efficient_time}")
        
        # ìµœì í™” ë¶„ì„ ìš”ì•½
        optimal_analysis = analysis_results.get("optimal_analysis", {})
        if optimal_analysis and optimal_analysis.get("best_timepart"):
            best_time = optimal_analysis["best_timepart"]
            summary["recommended_optimal_time"] = best_time
            summary["key_insights"].append(f"ì¢…í•© ì¶”ì²œ ìµœì  ì‹œê°„ëŒ€: {best_time}")
        
        return summary
    
    def save_batch_results(self, results: Dict[str, Any], filename_prefix: str = "3part_batch_optimization") -> str:
        """
        ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            results: ì €ì¥í•  ê²°ê³¼ ë°ì´í„°
            filename_prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data ë””ë ‰í„°ë¦¬ì— ì €ì¥
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
        data_dir = os.path.join(project_root, "data")
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return ""


def main():
    """ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ 3-Part API ìµœì í™” ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = ThreePartBatchProcessor()
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    results = processor.parallel_process_3part_data(date_range=7)
    
    # ì´ ì²˜ë¦¬ ì‹œê°„
    total_time = time.time() - start_time
    
    if results:
        print(f"\nâœ… 3-Part ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"ğŸ¯ ì„±ëŠ¥ ê°œì„ : ê¸°ì¡´ ëŒ€ë¹„ ì˜ˆìƒ 50% ë‹¨ì¶•")
        
        # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
        processing_info = results.get("processing_info", {})
        print(f"\nğŸ“ˆ ì²˜ë¦¬ ì •ë³´:")
        print(f"  - ì²˜ë¦¬ ë°©ì‹: {processing_info.get('processing_method', 'unknown')}")
        print(f"  - ìºì‹œ í™œìš©: {processing_info.get('cache_enabled', False)}")
        print(f"  - ì²˜ë¦¬ ë‚ ì§œ ë²”ìœ„: {processing_info.get('processed_date_range', 0)}ì¼")
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        summary = results.get("summary", {})
        if summary.get("key_insights"):
            print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
            for insight in summary["key_insights"]:
                print(f"  - {insight}")
        
        # ì‹œê°„ëŒ€ë³„ ìµœì í™” ì¶”ì²œ
        optimal_timeparts = results.get("optimal_timeparts", {})
        if optimal_timeparts.get("best_timepart"):
            print(f"\nğŸ¯ ìµœì í™” ì¶”ì²œ:")
            print(f"  - ìµœê³  ì„±ê³¼ ì‹œê°„ëŒ€: {optimal_timeparts['best_timepart']}")
            print(f"  - ê°œì„  í•„ìš” ì‹œê°„ëŒ€: {optimal_timeparts.get('worst_timepart', 'N/A')}")
            print(f"  - ì„±ê³¼ ê²©ì°¨: {optimal_timeparts.get('score_difference', 0):.2f}ì ")
        
        # ê²°ê³¼ ì €ì¥
        saved_file = processor.save_batch_results(results)
        if saved_file:
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {saved_file}")
    
    else:
        print("âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Task 6.1.1 (API í˜¸ì¶œ ìµœì í™”) êµ¬í˜„ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
