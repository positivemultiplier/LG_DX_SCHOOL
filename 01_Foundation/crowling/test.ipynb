{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3804e7a9",
   "metadata": {},
   "source": [
    "# 웹 크롤링(Web Crawling) 완전 정복 🕷️\n",
    "\n",
    "## 학습 목표\n",
    "- 웹 크롤링의 개념과 원리 이해\n",
    "- BeautifulSoup과 Selenium 라이브러리 활용\n",
    "- 정적/동적 웹페이지 크롤링 실습\n",
    "- 실제 웹사이트 데이터 수집 및 분석\n",
    "\n",
    "## 목차\n",
    "1. 웹 크롤링 기초 개념\n",
    "2. HTML 구조와 CSS 선택자\n",
    "3. BeautifulSoup을 이용한 정적 크롤링\n",
    "4. Selenium을 이용한 동적 크롤링\n",
    "5. 실전 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4750e",
   "metadata": {},
   "source": [
    "## 1. 웹 크롤링 기초 개념\n",
    "\n",
    "### 1.1 웹 크롤링이란?\n",
    "- **정의**: 웹페이지에서 원하는 데이터를 자동으로 수집하는 기술\n",
    "- **용도**: 데이터 분석, 가격 비교, 뉴스 수집, 시장 조사 등\n",
    "- **종류**:\n",
    "  - **정적 크롤링**: HTML이 고정된 웹페이지 (requests + BeautifulSoup)\n",
    "  - **동적 크롤링**: JavaScript로 생성되는 콘텐츠 (Selenium)\n",
    "\n",
    "### 1.2 크롤링 vs 스크래핑\n",
    "- **크롤링**: 웹페이지를 탐색하며 링크를 따라가는 과정\n",
    "- **스크래핑**: 특정 웹페이지에서 데이터를 추출하는 과정\n",
    "\n",
    "### 1.3 주의사항 ⚠️\n",
    "- **robots.txt** 확인: 웹사이트의 크롤링 정책 준수\n",
    "- **요청 간격**: 서버에 부하를 주지 않도록 적절한 딜레이\n",
    "- **법적/윤리적 고려**: 저작권, 개인정보보호법 준수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 설치\n",
    "# !pip install requests beautifulsoup4 lxml selenium pandas matplotlib seaborn\n",
    "\n",
    "# 라이브러리 임포트\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"라이브러리 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d131c3e",
   "metadata": {},
   "source": [
    "## 2. HTML 구조와 CSS 선택자\n",
    "\n",
    "### 2.1 HTML 기본 구조\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>페이지 제목</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1 id=\"title\">메인 제목</h1>\n",
    "        <p class=\"content\">내용</p>\n",
    "        <ul>\n",
    "            <li>항목 1</li>\n",
    "            <li>항목 2</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### 2.2 CSS 선택자\n",
    "- **태그 선택자**: `h1`, `p`, `div`\n",
    "- **클래스 선택자**: `.content`, `.container`\n",
    "- **ID 선택자**: `#title`\n",
    "- **자식 선택자**: `div > p` (직접 자식)\n",
    "- **후손 선택자**: `div p` (모든 하위)\n",
    "- **속성 선택자**: `[href]`, `[class=\"content\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup 기초 실습\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>크롤링 실습</title></head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1 id=\"main-title\">웹 크롤링 실습</h1>\n",
    "        <p class=\"description\">BeautifulSoup을 이용한 HTML 파싱</p>\n",
    "        <ul class=\"item-list\">\n",
    "            <li class=\"item\">항목 1</li>\n",
    "            <li class=\"item\">항목 2</li>\n",
    "            <li class=\"item\">항목 3</li>\n",
    "        </ul>\n",
    "        <div class=\"info\">\n",
    "            <span class=\"price\">15,000원</span>\n",
    "            <span class=\"discount\">20% 할인</span>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# BeautifulSoup 객체 생성\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 다양한 선택자 사용법\n",
    "print(\"=== BeautifulSoup 선택자 실습 ===\")\n",
    "print(f\"제목: {soup.find('h1').text}\")\n",
    "print(f\"설명: {soup.find('p', class_='description').text}\")\n",
    "print(f\"ID로 선택: {soup.find(id='main-title').text}\")\n",
    "\n",
    "# 여러 요소 선택\n",
    "items = soup.find_all('li', class_='item')\n",
    "print(f\"\\n항목들:\")\n",
    "for i, item in enumerate(items, 1):\n",
    "    print(f\"  {i}. {item.text}\")\n",
    "\n",
    "# CSS 선택자 사용\n",
    "price = soup.select_one('.info .price').text\n",
    "discount = soup.select_one('.info .discount').text\n",
    "print(f\"\\n가격 정보:\")\n",
    "print(f\"  가격: {price}\")\n",
    "print(f\"  할인: {discount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44972723",
   "metadata": {},
   "source": [
    "## 3. BeautifulSoup을 이용한 정적 크롤링\n",
    "\n",
    "### 3.1 실제 웹사이트 크롤링\n",
    "네이버 뉴스 헤드라인을 수집하는 실습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e196bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 헤드라인 크롤링 예제\n",
    "def crawl_naver_news():\n",
    "    \"\"\"네이버 뉴스 메인페이지에서 헤드라인 수집\"\"\"\n",
    "    url = \"https://news.naver.com/\"\n",
    "    \n",
    "    # User-Agent 설정 (서버가 차단하지 않도록)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 웹페이지 요청\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # HTTP 에러 체크\n",
    "        \n",
    "        # BeautifulSoup으로 파싱\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 뉴스 헤드라인 추출 (선택자는 실제 사이트 구조에 따라 변경 필요)\n",
    "        headlines = []\n",
    "        \n",
    "        # 메인 뉴스 영역에서 제목 추출\n",
    "        news_titles = soup.find_all('a', class_='cluster_text_headline')\n",
    "        \n",
    "        for title in news_titles[:10]:  # 상위 10개만\n",
    "            headlines.append({\n",
    "                'title': title.text.strip(),\n",
    "                'link': urljoin(url, title.get('href', ''))\n",
    "            })\n",
    "        \n",
    "        return headlines\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"요청 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "# 뉴스 헤드라인 수집 실행\n",
    "print(\"=== 네이버 뉴스 헤드라인 크롤링 ===\")\n",
    "news_data = crawl_naver_news()\n",
    "\n",
    "if news_data:\n",
    "    for i, news in enumerate(news_data, 1):\n",
    "        print(f\"{i}. {news['title']}\")\n",
    "        print(f\"   링크: {news['link'][:50]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"뉴스 데이터를 가져올 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58babb",
   "metadata": {},
   "source": [
    "## 4. Selenium을 이용한 동적 크롤링\n",
    "\n",
    "### 4.1 Selenium이란?\n",
    "- **정의**: 웹 브라우저를 자동화하는 도구\n",
    "- **장점**: JavaScript가 실행된 후의 DOM 상태 접근 가능\n",
    "- **사용 사례**: SPA(Single Page Application), AJAX 로딩, 버튼 클릭 등\n",
    "\n",
    "### 4.2 ChromeDriver 설정\n",
    "```bash\n",
    "# ChromeDriver 다운로드 필요\n",
    "# https://chromedriver.chromium.org/\n",
    "```\n",
    "\n",
    "### 4.3 주요 기능\n",
    "- **요소 찾기**: `find_element()`, `find_elements()`\n",
    "- **동작 수행**: 클릭, 텍스트 입력, 스크롤\n",
    "- **대기**: `implicitly_wait()`, `WebDriverWait()`\n",
    "- **스크린샷**: `save_screenshot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium 기본 사용법\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"ChromeDriver 설정\"\"\"\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  # 헤드리스 모드 (브라우저 창 안 띄움)\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.implicitly_wait(10)  # 10초 대기\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"드라이버 설정 오류: {e}\")\n",
    "        print(\"ChromeDriver가 설치되어 있는지 확인하세요.\")\n",
    "        return None\n",
    "\n",
    "# 구글 검색 자동화 예제\n",
    "def google_search_example(query):\n",
    "    \"\"\"구글에서 특정 키워드 검색\"\"\"\n",
    "    driver = setup_driver()\n",
    "    if not driver:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 구글 메인페이지 접속\n",
    "        driver.get(\"https://www.google.com\")\n",
    "        \n",
    "        # 검색창 찾기 및 검색어 입력\n",
    "        search_box = driver.find_element(By.NAME, \"q\")\n",
    "        search_box.send_keys(query)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "        \n",
    "        # 검색 결과 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"search\"))\n",
    "        )\n",
    "        \n",
    "        # 검색 결과 제목들 추출\n",
    "        results = driver.find_elements(By.CSS_SELECTOR, \"h3\")\n",
    "        \n",
    "        print(f\"=== '{query}' 검색 결과 (상위 5개) ===\")\n",
    "        for i, result in enumerate(results[:5], 1):\n",
    "            print(f\"{i}. {result.text}\")\n",
    "        \n",
    "        # 스크린샷 저장\n",
    "        driver.save_screenshot('google_search_result.png')\n",
    "        print(\"\\n스크린샷이 'google_search_result.png'로 저장되었습니다.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# 실행 (주석 해제하여 사용)\n",
    "# google_search_example(\"Python 크롤링\")\n",
    "print(\"Selenium 설정 완료! 위의 주석을 해제하여 구글 검색 예제를 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351bdb9",
   "metadata": {},
   "source": [
    "## 5. 실전 프로젝트: 쇼핑몰 상품 정보 크롤링\n",
    "\n",
    "### 5.1 프로젝트 개요\n",
    "- **목표**: 온라인 쇼핑몰에서 상품 정보 수집\n",
    "- **수집 데이터**: 상품명, 가격, 평점, 리뷰 수 등\n",
    "- **활용**: 가격 비교, 시장 분석, 데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56439f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쇼핑몰 상품 정보 크롤링 실습\n",
    "import random\n",
    "\n",
    "def crawl_product_info():\n",
    "    \"\"\"가상의 쇼핑몰 상품 정보 크롤링 (예시)\"\"\"\n",
    "    \n",
    "    # 실제 사이트 대신 가상 데이터 생성 (교육용)\n",
    "    products = []\n",
    "    categories = ['노트북', '스마트폰', '태블릿', '이어폰', '마우스']\n",
    "    brands = ['삼성', 'LG', '애플', '소니', '로지텍']\n",
    "    \n",
    "    for i in range(20):\n",
    "        product = {\n",
    "            'id': i + 1,\n",
    "            'name': f\"{random.choice(brands)} {random.choice(categories)} {i+1}\",\n",
    "            'price': random.randint(50000, 2000000),\n",
    "            'rating': round(random.uniform(3.0, 5.0), 1),\n",
    "            'reviews': random.randint(10, 1000),\n",
    "            'category': random.choice(categories)\n",
    "        }\n",
    "        products.append(product)\n",
    "    \n",
    "    return products\n",
    "\n",
    "# 상품 데이터 생성\n",
    "products_data = crawl_product_info()\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df_products = pd.DataFrame(products_data)\n",
    "print(\"=== 크롤링된 상품 정보 ===\")\n",
    "print(df_products.head(10))\n",
    "\n",
    "# 기본 통계 정보\n",
    "print(f\"\\n=== 데이터 요약 ===\")\n",
    "print(f\"총 상품 수: {len(df_products)}\")\n",
    "print(f\"평균 가격: {df_products['price'].mean():,.0f}원\")\n",
    "print(f\"평균 평점: {df_products['rating'].mean():.2f}\")\n",
    "print(f\"카테고리별 상품 수:\")\n",
    "print(df_products['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 데이터 시각화\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. 카테고리별 상품 수\n",
    "plt.subplot(2, 3, 1)\n",
    "df_products['category'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('카테고리별 상품 수')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. 가격 분포\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df_products['price'], bins=15, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "plt.title('상품 가격 분포')\n",
    "plt.xlabel('가격 (원)')\n",
    "plt.ylabel('상품 수')\n",
    "\n",
    "# 3. 평점 분포\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df_products['rating'], bins=10, color='orange', alpha=0.7, edgecolor='black')\n",
    "plt.title('상품 평점 분포')\n",
    "plt.xlabel('평점')\n",
    "plt.ylabel('상품 수')\n",
    "\n",
    "# 4. 카테고리별 평균 가격\n",
    "plt.subplot(2, 3, 4)\n",
    "category_price = df_products.groupby('category')['price'].mean().sort_values(ascending=False)\n",
    "category_price.plot(kind='bar', color='coral')\n",
    "plt.title('카테고리별 평균 가격')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('평균 가격 (원)')\n",
    "\n",
    "# 5. 가격 vs 평점 산점도\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(df_products['price'], df_products['rating'], alpha=0.6, color='purple')\n",
    "plt.title('가격 vs 평점')\n",
    "plt.xlabel('가격 (원)')\n",
    "plt.ylabel('평점')\n",
    "\n",
    "# 6. 리뷰 수 vs 평점\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(df_products['reviews'], df_products['rating'], alpha=0.6, color='red')\n",
    "plt.title('리뷰 수 vs 평점')\n",
    "plt.xlabel('리뷰 수')\n",
    "plt.ylabel('평점')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 상관관계 분석\n",
    "correlation = df_products[['price', 'rating', 'reviews']].corr()\n",
    "print(\"\\n=== 수치 데이터 상관관계 ===\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876ba24",
   "metadata": {},
   "source": [
    "## 6. 크롤링 모범 사례 및 팁 💡\n",
    "\n",
    "### 6.1 기술적 팁\n",
    "- **User-Agent 설정**: 실제 브라우저처럼 보이게 하기\n",
    "- **요청 간격 조절**: `time.sleep()` 사용으로 서버 부하 방지\n",
    "- **예외 처리**: `try-except` 구문으로 안정성 확보\n",
    "- **데이터 검증**: 수집된 데이터의 유효성 검사\n",
    "\n",
    "### 6.2 법적/윤리적 고려사항\n",
    "- **robots.txt 확인**: `사이트URL/robots.txt` 접확인\n",
    "- **이용약관 준수**: 웹사이트의 이용약관 검토\n",
    "- **개인정보 보호**: 개인정보는 수집하지 않기\n",
    "- **상업적 이용 주의**: 저작권 및 데이터 소유권 고려\n",
    "\n",
    "### 6.3 성능 최적화\n",
    "- **세션 재사용**: `requests.Session()` 활용\n",
    "- **병렬 처리**: `threading` 또는 `asyncio` 사용\n",
    "- **캐싱**: 중복 요청 방지\n",
    "- **효율적인 선택자**: CSS 선택자 최적화\n",
    "\n",
    "### 6.4 디버깅 팁\n",
    "- **개발자 도구 활용**: F12로 HTML 구조 분석\n",
    "- **단계별 테스트**: 작은 부분부터 점진적 개발\n",
    "- **로그 기록**: 진행 상황 및 오류 추적\n",
    "- **데이터 저장**: CSV, JSON 등으로 중간 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35505a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 유틸리티 함수들\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class CrawlingUtils:\n",
    "    \"\"\"크롤링에 유용한 유틸리티 함수들\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_to_csv(data, filename=None):\n",
    "        \"\"\"데이터를 CSV 파일로 저장\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"crawling_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        \n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        else:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"데이터가 '{filename}'에 저장되었습니다.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_to_json(data, filename=None):\n",
    "        \"\"\"데이터를 JSON 파일로 저장\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"crawling_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"데이터가 '{filename}'에 저장되었습니다.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"텍스트 정리 (공백, 특수문자 제거)\"\"\"\n",
    "        if text:\n",
    "            # 앞뒤 공백 제거\n",
    "            text = text.strip()\n",
    "            # 연속된 공백을 하나로 변경\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # 특수 문자 제거 (필요에 따라 수정)\n",
    "            text = re.sub(r'[^\\w\\s가-힣]', '', text)\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_numbers(text):\n",
    "        \"\"\"텍스트에서 숫자만 추출\"\"\"\n",
    "        if text:\n",
    "            numbers = re.findall(r'\\d+', text)\n",
    "            return ''.join(numbers)\n",
    "        return ''\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_url(url):\n",
    "        \"\"\"URL 유효성 검사\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# 유틸리티 함수 사용 예제\n",
    "print(\"=== 크롤링 유틸리티 함수 예제 ===\")\n",
    "\n",
    "# 텍스트 정리 예제\n",
    "sample_text = \"  삼성 갤럭시 S21   (5G)  \"\n",
    "clean_text = CrawlingUtils.clean_text(sample_text)\n",
    "print(f\"원본: '{sample_text}'\")\n",
    "print(f\"정리 후: '{clean_text}'\")\n",
    "\n",
    "# 숫자 추출 예제\n",
    "price_text = \"가격: 1,200,000원\"\n",
    "numbers = CrawlingUtils.extract_numbers(price_text)\n",
    "print(f\"가격 텍스트: '{price_text}'\")\n",
    "print(f\"숫자만: '{numbers}'\")\n",
    "\n",
    "# URL 검증 예제\n",
    "urls = [\"https://www.example.com\", \"invalid-url\", \"http://google.com\"]\n",
    "for url in urls:\n",
    "    is_valid = CrawlingUtils.validate_url(url)\n",
    "    print(f\"URL: {url} -> 유효: {is_valid}\")\n",
    "\n",
    "# 데이터 저장 예제 (위에서 생성한 products_data 사용)\n",
    "print(f\"\\n=== 데이터 저장 예제 ===\")\n",
    "CrawlingUtils.save_to_csv(df_products, \"products_sample.csv\")\n",
    "CrawlingUtils.save_to_json(products_data, \"products_sample.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e79b2",
   "metadata": {},
   "source": [
    "## 7. 수업 정리 및 연습 문제 📝\n",
    "\n",
    "### 7.1 학습 내용 요약\n",
    "1. **웹 크롤링 기초**: 정적 vs 동적, HTML 구조 이해\n",
    "2. **BeautifulSoup**: 정적 웹페이지에서 데이터 추출\n",
    "3. **Selenium**: 동적 웹페이지 자동화 및 데이터 수집\n",
    "4. **데이터 처리**: 수집된 데이터 정리 및 저장\n",
    "5. **시각화**: 크롤링 데이터를 차트로 표현\n",
    "6. **모범 사례**: 윤리적, 기술적 고려사항\n",
    "\n",
    "### 7.2 연습 문제 🚀\n",
    "\n",
    "#### 연습 1: 날씨 정보 크롤링\n",
    "- 기상청 또는 날씨 사이트에서 오늘의 날씨 정보 수집\n",
    "- 온도, 습도, 미세먼지 등의 정보 추출\n",
    "- 데이터프레임으로 정리하여 출력\n",
    "\n",
    "#### 연습 2: 주식 정보 수집\n",
    "- 네이버 금융에서 코스피 상위 10개 종목 정보 수집\n",
    "- 종목명, 현재가, 등락률 데이터 추출\n",
    "- 시각화를 통해 등락률 분석\n",
    "\n",
    "#### 연습 3: 도서 정보 크롤링\n",
    "- 온라인 서점에서 베스트셀러 목록 수집\n",
    "- 책 제목, 저자, 가격, 평점 정보 추출\n",
    "- 장르별 평균 가격 분석\n",
    "\n",
    "#### 연습 4: 부동산 정보 분석\n",
    "- 부동산 사이트에서 특정 지역 매물 정보 수집\n",
    "- 가격, 면적, 위치 정보 추출\n",
    "- 면적당 가격 계산 및 시각화\n",
    "\n",
    "### 7.3 추가 학습 자료\n",
    "- **공식 문서**: \n",
    "  - BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "  - Selenium: https://selenium-python.readthedocs.io/\n",
    "- **실습 사이트**: \n",
    "  - http://quotes.toscrape.com/ (크롤링 연습용)\n",
    "  - https://httpbin.org/ (HTTP 요청 테스트)\n",
    "- **고급 주제**: \n",
    "  - Scrapy 프레임워크\n",
    "  - 비동기 크롤링 (aiohttp)\n",
    "  - 분산 크롤링 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 실습: 간단한 크롤링 챌린지\n",
    "print(\"🎯 크롤링 수업 완료! 🎯\")\n",
    "print(\"\\n=== 수업에서 다룬 주요 기술들 ===\")\n",
    "print(\"✅ HTML 파싱 (BeautifulSoup)\")\n",
    "print(\"✅ CSS 선택자 활용\")\n",
    "print(\"✅ 웹 브라우저 자동화 (Selenium)\")\n",
    "print(\"✅ 데이터 정리 및 저장\")\n",
    "print(\"✅ 시각화를 통한 데이터 분석\")\n",
    "print(\"✅ 크롤링 모범 사례\")\n",
    "\n",
    "print(\"\\n=== 다음 단계 학습 추천 ===\")\n",
    "print(\"🔸 API 활용법 학습\")\n",
    "print(\"🔸 대용량 데이터 처리 (Pandas 고급)\")\n",
    "print(\"🔸 데이터베이스 연동\")\n",
    "print(\"🔸 머신러닝 데이터 전처리\")\n",
    "print(\"🔸 웹 애플리케이션 개발\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"    🚀 Happy Coding! 🚀\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 수업 완료 기념 간단한 통계\n",
    "lesson_stats = {\n",
    "    'topics_covered': 7,\n",
    "    'code_examples': 10,\n",
    "    'libraries_used': ['requests', 'beautifulsoup4', 'selenium', 'pandas', 'matplotlib'],\n",
    "    'practical_projects': 3\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 수업 통계:\")\n",
    "for key, value in lesson_stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# 학습자를 위한 체크리스트\n",
    "checklist = [\n",
    "    \"웹 크롤링의 기본 개념을 이해했나요?\",\n",
    "    \"BeautifulSoup으로 HTML을 파싱할 수 있나요?\",\n",
    "    \"Selenium으로 브라우저를 자동화할 수 있나요?\",\n",
    "    \"수집한 데이터를 정리하고 시각화할 수 있나요?\",\n",
    "    \"크롤링 시 주의사항을 알고 있나요?\"\n",
    "]\n",
    "\n",
    "print(f\"\\n✅ 학습 체크리스트:\")\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"   {i}. {item}\")\n",
    "\n",
    "print(f\"\\n🎓 수고하셨습니다! 크롤링 마스터가 되셨네요!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
